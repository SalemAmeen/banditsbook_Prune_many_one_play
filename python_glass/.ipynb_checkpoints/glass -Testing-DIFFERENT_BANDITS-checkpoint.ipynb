{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the testing data (Unseen data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using Theano backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.gridspec as gridspec\n",
    "%matplotlib inline\n",
    "from IPython.display import set_matplotlib_formats\n",
    "set_matplotlib_formats('png', 'pdf')\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Dropout\n",
    "import numpy\n",
    "from sklearn import metrics\n",
    "plt.rcParams['figure.figsize'] = (15, 6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training examples 136\n",
      "Number of validation examples 35\n",
      "Number of testing examples 43\n"
     ]
    }
   ],
   "source": [
    "X_train = np.load('./glass/X_train.npy')\n",
    "y_train = np.load('./glass/y_train.npy')\n",
    "X_test = np.load('./glass/X_test.npy')\n",
    "y_test = np.load('./glass/y_test.npy')\n",
    "X_deploy = np.load('./glass/X_deploy.npy')\n",
    "y_deploy = np.load('./glass/y_deploy.npy')\n",
    "\n",
    "print('Number of training examples',len(X_train))\n",
    "print('Number of validation examples',len(X_test))\n",
    "print('Number of testing examples',len(X_deploy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initialize the classifiers as the training time\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:695: UserWarning: Variables are collinear\n",
      "  warnings.warn(\"Variables are collinear\")\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "clf_neigh = KNeighborsClassifier(n_neighbors=5)\n",
    "clf_neigh.fit(X_train, y_train)\n",
    "from sklearn.svm import LinearSVC\n",
    "clf_svm_linear = LinearSVC(C=20.0)\n",
    "clf_svm_linear.fit(X_train, y_train)\n",
    "from sklearn.svm import SVC\n",
    "clf_svm = SVC(C=5.0, kernel='rbf')\n",
    "clf_svm.fit(X_train, y_train)\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "clf_dt = DecisionTreeClassifier(min_samples_split=2)\n",
    "clf_dt.fit(X_train, y_train)\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "clf_dt_IG = DecisionTreeClassifier(criterion='entropy', min_samples_split=2)\n",
    "clf_dt_IG.fit(X_train, y_train)\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "clf_dt_BGKN = BaggingClassifier(KNeighborsClassifier(n_neighbors=5),\n",
    "                             max_samples=0.5, max_features=0.5)\n",
    "clf_dt_BGKN.fit(X_train, y_train)\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "clf_dt_BGDT = BaggingClassifier(DecisionTreeClassifier(criterion='entropy', min_samples_split=2),\n",
    "                             max_samples=0.5, max_features=0.5)\n",
    "clf_dt_BGDT.fit(X_train, y_train)\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "clf_dt_RF = RandomForestClassifier()\n",
    "#Training\n",
    "start_time = time.time()\n",
    "clf_dt_RF.fit(X_train, y_train)\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "clf_dt_AD = AdaBoostClassifier()\n",
    "clf_dt_AD.fit(X_train, y_train)\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "clf_NB = GaussianNB()\n",
    "clf_NB.fit(X_train, y_train)\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "clf_dt_LDA = LinearDiscriminantAnalysis()\n",
    "clf_dt_LDA.fit(X_train, y_train)\n",
    "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n",
    "clf_dt_QDA = QuadraticDiscriminantAnalysis()\n",
    "clf_dt_QDA.fit(X_train, y_train)\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "LogReg = LogisticRegression(C=1e5)\n",
    "LogReg.fit(X_train, y_train)\n",
    "\n",
    "from sklearn.gaussian_process import GaussianProcessClassifier\n",
    "GP = GaussianProcessClassifier()\n",
    "GP.fit(X_train, y_train)\n",
    "\n",
    "import lightgbm as lgb\n",
    "lgb_train = lgb.Dataset(X_train, y_train)\n",
    "gbm = lgb.LGBMClassifier()\n",
    "gbm.fit(X_train, y_train, eval_metric=\"multi_logloss\")\n",
    "\n",
    "import xgboost as xgb\n",
    "XG_Boost = xgb.XGBClassifier(max_depth=3, n_estimators=300, learning_rate=0.05).fit(X_train, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/sklearn/cross_validation.py:44: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "import seaborn as sns\n",
    "import numpy as np\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout\n",
    "from keras.regularizers import l2\n",
    "from keras.utils import np_utils\n",
    "labelsTrain = np_utils.to_categorical(y_train)\n",
    "labelsTest = np_utils.to_categorical(y_test) \n",
    "labelsDepy = np_utils.to_categorical(y_deploy)                                                                                         \n",
    "model = Sequential()\n",
    "model.add(Dense(35,\n",
    "                input_shape=(9,), \n",
    "                activation=\"relu\",\n",
    "                W_regularizer=l2(0.001)))\n",
    "#model.add(Dropout(0.5))\n",
    "model.add(Dense(8, activation=\"softmax\"))\n",
    "model.load_weights('/Users/salemameen/Desktop/banditsbook/python_glass/glassModelbest.hdf5')\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'],\n",
    "              optimizer='adam')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# UCB1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# MAB_NN(4%REMOVED) MODEL1    1               \n",
    "model1 = Sequential()\n",
    "model1.add(Dense(35,\n",
    "                input_shape=(9,), \n",
    "                activation=\"relu\",\n",
    "                W_regularizer=l2(0.001)))\n",
    "#model.add(Dropout(0.5))\n",
    "model1.add(Dense(8, activation=\"softmax\"))\n",
    "model1.load_weights('./UCB1/spam5.hdf5')\n",
    "model1.compile(loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'],\n",
    "              optimizer='adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# MAB_NN(36%REMOVED) MODEL2 9\n",
    "model2 = Sequential()\n",
    "model2.add(Dense(35,\n",
    "                input_shape=(9,), \n",
    "                activation=\"relu\",\n",
    "                W_regularizer=l2(0.001)))\n",
    "#model.add(Dropout(0.5))\n",
    "model2.add(Dense(8, activation=\"softmax\"))\n",
    "model2.load_weights('./UCB1/spam8.hdf5')\n",
    "model2.compile(loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'],\n",
    "              optimizer='adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# MAB_NN(60%REMOVED) MODEL3 15\n",
    "model3 = Sequential()\n",
    "model3.add(Dense(35,\n",
    "                input_shape=(9,), \n",
    "                activation=\"relu\",\n",
    "                W_regularizer=l2(0.001)))\n",
    "#model.add(Dropout(0.5))\n",
    "model3.add(Dense(8, activation=\"softmax\"))\n",
    "model3.load_weights('./UCB1/spam14.hdf5')\n",
    "model3.compile(loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'],\n",
    "              optimizer='adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# MAB_NN(85%REMOVED) MODEL3 15\n",
    "model4 = Sequential()\n",
    "model4.add(Dense(35,\n",
    "                input_shape=(9,), \n",
    "                activation=\"relu\",\n",
    "                W_regularizer=l2(0.001)))\n",
    "#model.add(Dropout(0.5))\n",
    "model4.add(Dense(8, activation=\"softmax\"))\n",
    "model4.load_weights('./UCB1/spam20.hdf5')\n",
    "model4.compile(loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'],\n",
    "              optimizer='adam')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Espsilon Greedy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model_EG = Sequential()\n",
    "model_EG.add(Dense(35,\n",
    "                input_shape=(9,), \n",
    "                activation=\"relu\",\n",
    "                W_regularizer=l2(0.001)))\n",
    "#model.add(Dropout(0.5))\n",
    "model_EG.add(Dense(8, activation=\"softmax\"))\n",
    "model_EG.load_weights('./EpsilonGreedy/spam5.hdf5')\n",
    "model_EG.compile(loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'],\n",
    "              optimizer='adam')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Annealing Epsilon Greedy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model_AEG = Sequential()\n",
    "model_AEG.add(Dense(35,\n",
    "                input_shape=(9,), \n",
    "                activation=\"relu\",\n",
    "                W_regularizer=l2(0.001)))\n",
    "#model.add(Dropout(0.5))\n",
    "model_AEG.add(Dense(8, activation=\"softmax\"))\n",
    "model_AEG.load_weights('./AnnealingEpsilonGreedy/spam5.hdf5')\n",
    "model_AEG.compile(loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'],\n",
    "              optimizer='adam')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SOFTMAX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model_SM = Sequential()\n",
    "model_SM.add(Dense(35,\n",
    "                input_shape=(9,), \n",
    "                activation=\"relu\",\n",
    "                W_regularizer=l2(0.001)))\n",
    "#model.add(Dropout(0.5))\n",
    "model_SM.add(Dense(8, activation=\"softmax\"))\n",
    "model_SM.load_weights('./Softmax/spam5.hdf5')\n",
    "model_SM.compile(loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'],\n",
    "              optimizer='adam')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ANNEELYING SOFTMAX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model_ASM = Sequential()\n",
    "model_ASM.add(Dense(35,\n",
    "                input_shape=(9,), \n",
    "                activation=\"relu\",\n",
    "                W_regularizer=l2(0.001)))\n",
    "#model.add(Dropout(0.5))\n",
    "model_ASM.add(Dense(8, activation=\"softmax\"))\n",
    "model_ASM.load_weights('./AnnealingSoftmax/spam5.hdf5')\n",
    "model_ASM.compile(loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'],\n",
    "              optimizer='adam')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## THOMPSON SAMBLING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model_TS = Sequential()\n",
    "model_TS.add(Dense(35,\n",
    "                input_shape=(9,), \n",
    "                activation=\"relu\",\n",
    "                W_regularizer=l2(0.001)))\n",
    "#model.add(Dropout(0.5))\n",
    "model_TS.add(Dense(8, activation=\"softmax\"))\n",
    "model_TS.load_weights('./thompson_sampling/spam5.hdf5')\n",
    "model_TS.compile(loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'],\n",
    "              optimizer='adam')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HEDGE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model_HG = Sequential()\n",
    "model_HG.add(Dense(35,\n",
    "                input_shape=(9,), \n",
    "                activation=\"relu\",\n",
    "                W_regularizer=l2(0.001)))\n",
    "#model.add(Dropout(0.5))\n",
    "model_HG.add(Dense(8, activation=\"softmax\"))\n",
    "model_HG.load_weights('./Hedge/spam5.hdf5')\n",
    "model_HG.compile(loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'],\n",
    "              optimizer='adam')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EXP3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model_EXP = Sequential()\n",
    "model_EXP.add(Dense(35,\n",
    "                input_shape=(9,), \n",
    "                activation=\"relu\",\n",
    "                W_regularizer=l2(0.001)))\n",
    "#model.add(Dropout(0.5))\n",
    "model_EXP.add(Dense(8, activation=\"softmax\"))\n",
    "model_EXP.load_weights('./Exp3/spam5.hdf5')\n",
    "model_EXP.compile(loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'],\n",
    "              optimizer='adam')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deployment Time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The time of KNN is                       0.0015158653259277344 seconds\n",
      "The time of LSVM is                      0.00020313262939453125 seconds \n",
      "The time of SVM is                       0.00035309791564941406 seconds \n",
      "The time of DT_gini is                   0.00017786026000976562 seconds \n",
      "The time of DT is                        0.00015020370483398438 seconds\n",
      "The time of Bagging Knn is               0.005739927291870117 seconds\n",
      "The time of Bagging DT is                 0.001065969467163086 seconds \n",
      "The time of Random Forest is              0.0010151863098144531 seconds \n",
      "The time of Ada Boost is                  0.004230022430419922 seconds \n",
      "The time of Naive Bayes is                 0.0003497600555419922 seconds \n",
      "The time of LDA is                         0.0002319812774658203 seconds \n",
      "The time of QDA is                         0.0003371238708496094 seconds \n",
      "The time of Logistic Regression is         0.00017189979553222656 seconds \n",
      "The time of Gaussian Process Classifier is  0.006225109100341797 seconds \n",
      "The time of LightGBM is                     0.0006210803985595703 seconds \n",
      "The time of xgboost is                     0.0012180805206298828 seconds \n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Error when checking : expected dense_input_1 to have shape (None, 11) but got array with shape (43, 9)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-16-0b675ae106f2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     64\u001b[0m \u001b[0;31m# NN\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m \u001b[0mstart_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 66\u001b[0;31m \u001b[0my_pred_NN\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict_classes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_deploy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     67\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"The time of NN is                           %s seconds \"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mstart_time\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m \u001b[0;31m# NN 2.5% removed neurals\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/keras/models.py\u001b[0m in \u001b[0;36mpredict_classes\u001b[0;34m(self, x, batch_size, verbose)\u001b[0m\n\u001b[1;32m    824\u001b[0m             \u001b[0mA\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0marray\u001b[0m \u001b[0mof\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mpredictions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    825\u001b[0m         '''\n\u001b[0;32m--> 826\u001b[0;31m         \u001b[0mproba\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    827\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mproba\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    828\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mproba\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/keras/models.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, x, batch_size, verbose)\u001b[0m\n\u001b[1;32m    714\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    715\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuild\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 716\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    717\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    718\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mpredict_on_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, x, batch_size, verbose)\u001b[0m\n\u001b[1;32m   1199\u001b[0m         x = standardize_input_data(x, self.input_names,\n\u001b[1;32m   1200\u001b[0m                                    \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minternal_input_shapes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1201\u001b[0;31m                                    check_batch_dim=False)\n\u001b[0m\u001b[1;32m   1202\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstateful\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1203\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mbatch_size\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mbatch_size\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mstandardize_input_data\u001b[0;34m(data, names, shapes, check_batch_dim, exception_prefix)\u001b[0m\n\u001b[1;32m    111\u001b[0m                             \u001b[0;34m' to have shape '\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshapes\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    112\u001b[0m                             \u001b[0;34m' but got array with shape '\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 113\u001b[0;31m                             str(array.shape))\n\u001b[0m\u001b[1;32m    114\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0marrays\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Error when checking : expected dense_input_1 to have shape (None, 11) but got array with shape (43, 9)"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "y_pred_neigh = clf_neigh.predict(X_deploy)\n",
    "print(\"The time of KNN is                       %s seconds\" % (time.time() - start_time))\n",
    "# Linear SVM \n",
    "start_time = time.time()\n",
    "y_pred_svm_linear = clf_svm_linear.predict(X_deploy)\n",
    "print(\"The time of LSVM is                      %s seconds \" % (time.time() - start_time))\n",
    "# SVM kernel='rbf'\n",
    "start_time = time.time()\n",
    "y_pred_svm = clf_svm.predict(X_deploy)\n",
    "print(\"The time of SVM is                       %s seconds \" % (time.time() - start_time))\n",
    "# DT with gini\n",
    "start_time = time.time()\n",
    "y_pred_gtgini = clf_dt.predict(X_deploy)\n",
    "print(\"The time of DT_gini is                   %s seconds \" % (time.time() - start_time))\n",
    "# DT with entorpy\n",
    "start_time = time.time()\n",
    "y_pred_dt_IG = clf_dt_IG.predict(X_deploy)\n",
    "print(\"The time of DT is                        %s seconds\" % (time.time() - start_time))\n",
    "# Bagging with Knn\n",
    "start_time = time.time()\n",
    "y_pred_dt_BGKN = clf_dt_BGKN.predict(X_deploy)\n",
    "print(\"The time of Bagging Knn is               %s seconds\" % (time.time() - start_time))\n",
    "# Bagging with DT with gini\n",
    "start_time = time.time()\n",
    "y_pred_dt_BGDT = clf_dt_BGDT.predict(X_deploy)\n",
    "print(\"The time of Bagging DT is                 %s seconds \" % (time.time() - start_time))\n",
    "# Random Forest with DT with gini \n",
    "start_time = time.time()\n",
    "y_pred_dt_RF = clf_dt_RF.predict(X_deploy)\n",
    "print(\"The time of Random Forest is              %s seconds \" % (time.time() - start_time))\n",
    "# Ada Boost\n",
    "start_time = time.time()\n",
    "y_pred_dt_AD = clf_dt_AD.predict(X_deploy)\n",
    "print(\"The time of Ada Boost is                  %s seconds \" % (time.time() - start_time))\n",
    "# Naive Bayes\n",
    "start_time = time.time()\n",
    "y_pred_NB = clf_NB.predict(X_deploy)\n",
    "print(\"The time of Naive Bayes is                 %s seconds \" % (time.time() - start_time))\n",
    "# LDA\n",
    "start_time = time.time()\n",
    "y_pred_dt_LDA = clf_dt_LDA.predict(X_deploy)\n",
    "print(\"The time of LDA is                         %s seconds \" % (time.time() - start_time))\n",
    "# QDA\n",
    "start_time = time.time()\n",
    "y_pred_dt_QDA = clf_dt_QDA.predict(X_deploy)\n",
    "print(\"The time of QDA is                         %s seconds \" % (time.time() - start_time))\n",
    "# Logistic Regression\n",
    "start_time = time.time()\n",
    "y_pred_LR = LogReg.predict(X_deploy)\n",
    "print(\"The time of Logistic Regression is         %s seconds \" % (time.time() - start_time))\n",
    "# Gaussian Process Classifier\n",
    "start_time = time.time()\n",
    "y_pred_GP = GP.predict(X_deploy)\n",
    "print(\"The time of Gaussian Process Classifier is  %s seconds \" % (time.time() - start_time))\n",
    "# LightGBM\n",
    "start_time = time.time()\n",
    "y_pred_gbm = gbm.predict(X_deploy)\n",
    "print(\"The time of LightGBM is                     %s seconds \" % (time.time() - start_time))\n",
    "# xgboost\n",
    "start_time = time.time()\n",
    "y_pred_xgboost = XG_Boost.predict(X_deploy)\n",
    "print(\"The time of xgboost is                     %s seconds \" % (time.time() - start_time))\n",
    "# NN\n",
    "start_time = time.time()\n",
    "y_pred_NN = model.predict_classes(X_deploy, verbose=0)\n",
    "print(\"The time of NN is                           %s seconds \" % (time.time() - start_time))\n",
    "# NN 2.5% removed neurals\n",
    "start_time = time.time()\n",
    "y_pred_NN1 = model1.predict_classes(X_deploy, verbose=0)\n",
    "print(\"The time of NN2.5 Removed is               %s seconds \" % (time.time() - start_time))\n",
    "# NN 7.5% removed neurals\n",
    "start_time = time.time()\n",
    "y_pred_NN2 = model2.predict_classes(X_deploy, verbose=0)\n",
    "print(\"The time of NN7.5 Removed is               %s seconds \" % (time.time() - start_time))\n",
    "# NN 65% removed neurals\n",
    "start_time = time.time()\n",
    "y_pred_NN3 = model3.predict_classes(X_deploy, verbose=0)\n",
    "print(\"The time of NN65 Removed is                %s seconds \" % (time.time() - start_time))\n",
    "# NN 85% removed neurals\n",
    "start_time = time.time()\n",
    "y_pred_NN4 = model4.predict_classes(X_deploy, verbose=0)\n",
    "print(\"The time of NN85 Removed is                %s seconds \" % (time.time() - start_time))\n",
    "# NN 20% removed neurals USING EPSILON GREEDY\n",
    "start_time = time.time()\n",
    "y_pred_EG = model_EG.predict_classes(X_deploy, verbose=0)\n",
    "print(\"The time of EPSILON GREEDY Removed is      %s seconds \" % (time.time() - start_time))\n",
    "# NN 20% removed neurals USING ANNEELYING EPSILON GREEDY\n",
    "start_time = time.time()\n",
    "y_pred_AEG = model_AEG.predict_classes(X_deploy, verbose=0)\n",
    "print(\"The time of DECAYING EPSILON GREEDY Removed is  %s seconds \" % (time.time() - start_time))\n",
    "# NN 20% removed neurals USING SOFTMAX\n",
    "start_time = time.time()\n",
    "y_pred_SM = model_SM.predict_classes(X_deploy, verbose=0)\n",
    "print(\"The time of SOFTMAX Removed is                  %s seconds \" % (time.time() - start_time))\n",
    "# NN 20% removed neurals USING DECAYING SOFTMAX\n",
    "start_time = time.time()\n",
    "y_pred_ASM = model_ASM.predict_classes(X_deploy, verbose=0)\n",
    "print(\"The time of DECAYING SOFTMAX Removed is         %s seconds \" % (time.time() - start_time))\n",
    "# NN 20% removed neurals USING THOMPSON SAMPLING\n",
    "start_time = time.time()\n",
    "y_pred_TS = model_TS.predict_classes(X_deploy, verbose=0)\n",
    "print(\"The time of THOMPSON SAMPLING Removed is        %s seconds \" % (time.time() - start_time))\n",
    "# NN 20% removed neurals USING HEDGE\n",
    "start_time = time.time()\n",
    "y_pred_HG = model_HG.predict_classes(X_deploy, verbose=0)\n",
    "print(\"The time of HEDGE Removed is                    %s seconds \" % (time.time() - start_time))\n",
    "# NN 20% removed neurals USING EXP3\n",
    "start_time = time.time()\n",
    "y_pred_EXP = model_EXP.predict_classes(X_deploy, verbose=0)\n",
    "print(\"The time of EXP3 Removed is                    %s seconds \" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Accuracy of the models on Testing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(\"The accuracy of KNN:          \\t\", metrics.accuracy_score(y_deploy, y_pred_neigh))\n",
    "print(\"The accuracy of LSVM:         \\t\", metrics.accuracy_score(y_deploy, y_pred_svm_linear))\n",
    "print(\"The accuracy of SVM:          \\t\", metrics.accuracy_score(y_deploy, y_pred_svm))\n",
    "print(\"The accuracy of DT_gini:      \\t\", metrics.accuracy_score(y_deploy, y_pred_gtgini))\n",
    "print(\"The accuracy of DT_entorpy:   \\t\", metrics.accuracy_score(y_deploy, y_pred_dt_IG))\n",
    "print(\"The accuracy of Bagging Knn:  \\t\", metrics.accuracy_score(y_deploy, y_pred_dt_BGKN))\n",
    "print(\"The accuracy of Bagging DT:   \\t\", metrics.accuracy_score(y_deploy, y_pred_dt_BGDT))\n",
    "print(\"The accuracy of Random Forest:\\t\", metrics.accuracy_score(y_deploy, y_pred_dt_RF))\n",
    "print(\"The accuracy of Ada Boost:    \\t\", metrics.accuracy_score(y_deploy, y_pred_dt_AD))\n",
    "print(\"The accuracy of Naive Bayes:  \\t\", metrics.accuracy_score(y_deploy, y_pred_NB))\n",
    "print(\"The accuracy of LDA:          \\t\", metrics.accuracy_score(y_deploy, y_pred_dt_LDA))\n",
    "print(\"The accuracy of QDA:          \\t\", metrics.accuracy_score(y_deploy, y_pred_dt_QDA))\n",
    "print(\"The accuracy of Log. Reg.:    \\t\", metrics.accuracy_score(y_deploy, y_pred_LR))\n",
    "print(\"The accuracy of GP CLASSI. :  \\t\", metrics.accuracy_score(y_deploy, y_pred_GP))\n",
    "print(\"The accuracy of LightGBM:     \\t\", metrics.accuracy_score(y_deploy, y_pred_gbm))\n",
    "print(\"The accuracy of Xgboost:      \\t\", metrics.accuracy_score(y_deploy, y_pred_xgboost))\n",
    "print(\"The accuracy of NN:           \\t\", metrics.accuracy_score(y_deploy, y_pred_NN))\n",
    "print(\"The accuracy of NN2.5R:       \\t\", metrics.accuracy_score(y_deploy, y_pred_NN1))\n",
    "print(\"The accuracy of NN7.5R:       \\t\", metrics.accuracy_score(y_deploy, y_pred_NN2))\n",
    "print(\"The accuracy of NN65R:        \\t\", metrics.accuracy_score(y_deploy, y_pred_NN3))\n",
    "print(\"The accuracy of NN85R:        \\t\", metrics.accuracy_score(y_deploy, y_pred_NN4))\n",
    "print(\"The accuracy of UCB1 NN20R:    \\t\", metrics.accuracy_score(y_deploy, y_pred_NN1))\n",
    "print(\"The accuracy of E GREEDY  :    \\t\", metrics.accuracy_score(y_deploy, y_pred_EG))\n",
    "print(\"The accuracy of A e Greegy:    \\t\", metrics.accuracy_score(y_deploy, y_pred_AEG))\n",
    "print(\"The accuracy of SOFTMAX :      \\t\", metrics.accuracy_score(y_deploy, y_pred_SM))\n",
    "print(\"The accuracy of A SOFTMAX:     \\t\", metrics.accuracy_score(y_deploy, y_pred_ASM))\n",
    "print(\"The accuracy of THOMPSON SAM.: \\t\", metrics.accuracy_score(y_deploy, y_pred_TS))\n",
    "print(\"The accuracy of HEDGE:         \\t\", metrics.accuracy_score(y_deploy, y_pred_HG))\n",
    "print(\"The accuracy of EXP3 :         \\t\", metrics.accuracy_score(y_deploy, y_pred_EXP))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Precision of the models on Testing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(\"Precision of KNN:          \\t\", metrics.precision_score(y_deploy, y_pred_neigh, average=\"macro\"))\n",
    "print(\"Precision of LSVM:         \\t\", metrics.precision_score(y_deploy, y_pred_svm_linear, average=\"macro\"))\n",
    "print(\"Precision of SVM:          \\t\", metrics.precision_score(y_deploy, y_pred_svm, average=\"macro\"))\n",
    "print(\"Precision of DT_gini:      \\t\", metrics.precision_score(y_deploy, y_pred_gtgini, average=\"macro\"))\n",
    "print(\"Precision of DT_entorpy:   \\t\", metrics.precision_score(y_deploy, y_pred_dt_IG, average=\"macro\"))\n",
    "print(\"Precision of Bagging Knn:  \\t\", metrics.precision_score(y_deploy, y_pred_dt_BGKN, average=\"macro\"))\n",
    "print(\"Precision of Bagging DT:   \\t\", metrics.precision_score(y_deploy, y_pred_dt_BGDT, average=\"macro\"))\n",
    "print(\"Precision of Random Forest:\\t\", metrics.precision_score(y_deploy, y_pred_dt_RF, average=\"macro\"))\n",
    "print(\"Precision of Ada Boost:    \\t\", metrics.precision_score(y_deploy, y_pred_dt_AD, average=\"macro\"))\n",
    "print(\"Precision of Naive Bayes:  \\t\", metrics.precision_score(y_deploy, y_pred_NB, average=\"macro\"))\n",
    "print(\"Precision of LDA:          \\t\", metrics.precision_score(y_deploy, y_pred_dt_LDA, average=\"macro\"))\n",
    "print(\"Precision of QDA:          \\t\", metrics.precision_score(y_deploy, y_pred_dt_QDA, average=\"macro\"))\n",
    "print(\"Precision of Log. Reg.:    \\t\", metrics.precision_score(y_deploy, y_pred_LR, average=\"macro\"))\n",
    "print(\"Precision of GP CLASSI. :  \\t\", metrics.precision_score(y_deploy, y_pred_GP, average=\"macro\"))\n",
    "print(\"Precision of LightGBM:     \\t\", metrics.precision_score(y_deploy, y_pred_gbm, average=\"macro\"))\n",
    "print(\"Precision of Xgboost:      \\t\", metrics.precision_score(y_deploy, y_pred_xgboost, average=\"macro\"))\n",
    "print(\"Precision of NN:           \\t\", metrics.precision_score(y_deploy, y_pred_NN, average=\"macro\"))\n",
    "print(\"Precision of NN2.5R:       \\t\", metrics.precision_score(y_deploy, y_pred_NN1, average=\"macro\"))\n",
    "print(\"Precision of NN7.5R:       \\t\", metrics.precision_score(y_deploy, y_pred_NN2, average=\"macro\"))\n",
    "print(\"Precision of NN65R:        \\t\", metrics.precision_score(y_deploy, y_pred_NN3, average=\"macro\"))\n",
    "print(\"Precision of NN85R:        \\t\", metrics.precision_score(y_deploy, y_pred_NN4, average=\"macro\"))\n",
    "print(\"Precision of UCB1 NN20R:    \\t\", metrics.precision_score(y_deploy, y_pred_NN1, average=\"macro\"))\n",
    "print(\"Precision of E GREEDY  :    \\t\", metrics.precision_score(y_deploy, y_pred_EG, average=\"macro\"))\n",
    "print(\"Precision of A e Greegy:    \\t\", metrics.precision_score(y_deploy, y_pred_AEG, average=\"macro\"))\n",
    "print(\"Precision of SOFTMAX :      \\t\", metrics.precision_score(y_deploy, y_pred_SM, average=\"macro\"))\n",
    "print(\"Precision of A SOFTMAX:     \\t\", metrics.precision_score(y_deploy, y_pred_ASM, average=\"macro\"))\n",
    "print(\"Precision of THOMPSON SAM.: \\t\", metrics.precision_score(y_deploy, y_pred_TS, average=\"macro\"))\n",
    "print(\"Precision of HEDGE:         \\t\", metrics.precision_score(y_deploy, y_pred_HG, average=\"macro\"))\n",
    "print(\"Precision of EXP3 :         \\t\", metrics.precision_score(y_deploy, y_pred_EXP, average=\"macro\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recall of the models on Testing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(\"Recall of KNN :         \\t\", metrics.recall_score(y_deploy, y_pred_neigh, average=\"macro\"))\n",
    "print(\"Recall of LSVM :        \\t\", metrics.recall_score(y_deploy, y_pred_svm_linear, average=\"macro\"))\n",
    "print(\"Recall of SVM :         \\t\", metrics.recall_score(y_deploy, y_pred_svm, average=\"macro\"))\n",
    "print(\"Recall of DT_gini :     \\t\", metrics.recall_score(y_deploy, y_pred_gtgini, average=\"macro\"))\n",
    "print(\"Recall of DT_entorpy:   \\t\", metrics.recall_score(y_deploy, y_pred_dt_IG, average=\"macro\"))\n",
    "print(\"Recall of Bagging Knn : \\t\", metrics.recall_score(y_deploy, y_pred_dt_BGKN, average=\"macro\"))\n",
    "print(\"Recall of Bagging DT :  \\t\", metrics.recall_score(y_deploy, y_pred_dt_BGDT, average=\"macro\"))\n",
    "print(\"Recall of Random Forest:\\t\", metrics.recall_score(y_deploy, y_pred_dt_RF, average=\"macro\"))\n",
    "print(\"Recall of Ada Boost :   \\t\", metrics.recall_score(y_deploy, y_pred_dt_AD, average=\"macro\"))\n",
    "print(\"Recall of Naive Bayes : \\t\", metrics.recall_score(y_deploy, y_pred_NB, average=\"macro\"))\n",
    "print(\"Recall of LDA :         \\t\", metrics.recall_score(y_deploy, y_pred_dt_LDA, average=\"macro\"))\n",
    "print(\"Recall of QDA :         \\t\", metrics.recall_score(y_deploy, y_pred_dt_QDA, average=\"macro\"))\n",
    "print(\"Recall of Log. Reg.:    \\t\", metrics.recall_score(y_deploy, y_pred_LR, average=\"macro\"))\n",
    "print(\"Recall of GP CLASSI. :  \\t\", metrics.recall_score(y_deploy, y_pred_GP, average=\"macro\"))\n",
    "print(\"Recall of LightGBM:     \\t\", metrics.recall_score(y_deploy, y_pred_gbm, average=\"macro\"))\n",
    "print(\"Recall of Xgboost:      \\t\", metrics.recall_score(y_deploy, y_pred_xgboost, average=\"macro\"))\n",
    "print(\"Recall of NN :          \\t\", metrics.recall_score(y_deploy, y_pred_NN, average=\"macro\"))\n",
    "print(\"Recall of NN2.5R:       \\t\", metrics.recall_score(y_deploy, y_pred_NN1, average=\"macro\"))\n",
    "print(\"Recall of NN7.5R:       \\t\", metrics.recall_score(y_deploy, y_pred_NN2, average=\"macro\"))\n",
    "print(\"Recall of NN65R:        \\t\", metrics.recall_score(y_deploy, y_pred_NN3, average=\"macro\"))\n",
    "print(\"Recall of NN85R:        \\t\", metrics.recall_score(y_deploy, y_pred_NN4, average=\"macro\"))\n",
    "print(\"Recall of UCB1 NN20R:    \\t\", metrics.recall_score(y_deploy, y_pred_NN1, average=\"macro\"))\n",
    "print(\"Recall of E GREEDY  :    \\t\", metrics.recall_score(y_deploy, y_pred_EG, average=\"macro\"))\n",
    "print(\"Recall of A e Greegy:    \\t\", metrics.recall_score(y_deploy, y_pred_AEG, average=\"macro\"))\n",
    "print(\"Recall of SOFTMAX :      \\t\", metrics.recall_score(y_deploy, y_pred_SM, average=\"macro\"))\n",
    "print(\"Recall of A SOFTMAX:     \\t\", metrics.recall_score(y_deploy, y_pred_ASM, average=\"macro\"))\n",
    "print(\"Recall of THOMPSON SAM.: \\t\", metrics.recall_score(y_deploy, y_pred_TS, average=\"macro\"))\n",
    "print(\"Recall of HEDGE:         \\t\", metrics.recall_score(y_deploy, y_pred_HG, average=\"macro\"))\n",
    "print(\"Recall of EXP3 :         \\t\", metrics.recall_score(y_deploy, y_pred_EXP, average=\"macro\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# F1 score of the models on Testing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(\"F1 score of KNN:          \\t\", metrics.f1_score(y_deploy, y_pred_neigh, average=\"macro\"))\n",
    "print(\"F1 score of LSVM:         \\t\", metrics.f1_score(y_deploy, y_pred_svm_linear, average=\"macro\"))\n",
    "print(\"F1 score of SVM:          \\t\", metrics.f1_score(y_deploy, y_pred_svm, average=\"macro\"))\n",
    "print(\"F1 score of DT_gini:      \\t\", metrics.f1_score(y_deploy, y_pred_gtgini, average=\"macro\"))\n",
    "print(\"F1 score of DT_entorpy:   \\t\", metrics.f1_score(y_deploy, y_pred_dt_IG, average=\"macro\"))\n",
    "print(\"F1 score of Bagging Knn:  \\t\", metrics.f1_score(y_deploy, y_pred_dt_BGKN, average=\"macro\"))\n",
    "print(\"F1 score of Bagging DT:   \\t\", metrics.f1_score(y_deploy, y_pred_dt_BGDT, average=\"macro\"))\n",
    "print(\"F1 score of Random Forest:\\t\", metrics.f1_score(y_deploy, y_pred_dt_RF, average=\"macro\"))\n",
    "print(\"F1 score of Ada Boost:    \\t\", metrics.f1_score(y_deploy, y_pred_dt_AD, average=\"macro\"))\n",
    "print(\"F1 score of Naive Bayes:  \\t\", metrics.f1_score(y_deploy, y_pred_NB, average=\"macro\"))\n",
    "print(\"F1 score of LDA:          \\t\", metrics.f1_score(y_deploy, y_pred_dt_LDA, average=\"macro\"))\n",
    "print(\"F1 score of QDA:          \\t\", metrics.f1_score(y_deploy, y_pred_dt_QDA, average=\"macro\"))\n",
    "print(\"F1 score of Log. Reg.:    \\t\", metrics.f1_score(y_deploy, y_pred_LR, average=\"macro\"))\n",
    "print(\"F1 score of GP CLASSI. :  \\t\", metrics.f1_score(y_deploy, y_pred_GP, average=\"macro\"))\n",
    "print(\"F1 score of LightGBM:     \\t\", metrics.f1_score(y_deploy, y_pred_gbm, average=\"macro\"))\n",
    "print(\"F1 score of Xgboost:      \\t\", metrics.f1_score(y_deploy, y_pred_xgboost, average=\"macro\"))\n",
    "print(\"F1 score of NN:           \\t\", metrics.f1_score(y_deploy, y_pred_NN, average=\"macro\"))\n",
    "print(\"F1 score of NN2.5R:       \\t\", metrics.f1_score(y_deploy, y_pred_NN1, average=\"macro\"))\n",
    "print(\"F1 score of NN7.5R:       \\t\", metrics.f1_score(y_deploy, y_pred_NN2, average=\"macro\"))\n",
    "print(\"F1 score of NN65R:        \\t\", metrics.f1_score(y_deploy, y_pred_NN3, average=\"macro\"))\n",
    "print(\"F1 score of NN85R:        \\t\", metrics.f1_score(y_deploy, y_pred_NN4, average=\"macro\"))\n",
    "print(\"F1 score of UCB1 NN20R:    \\t\", metrics.f1_score(y_deploy, y_pred_NN1, average=\"macro\"))\n",
    "print(\"F1 score of E GREEDY  :    \\t\", metrics.f1_score(y_deploy, y_pred_EG, average=\"macro\"))\n",
    "print(\"F1 score of A e Greegy:    \\t\", metrics.f1_score(y_deploy, y_pred_AEG, average=\"macro\"))\n",
    "print(\"F1 score of SOFTMAX :      \\t\", metrics.f1_score(y_deploy, y_pred_SM, average=\"macro\"))\n",
    "print(\"F1 score of A SOFTMAX:     \\t\", metrics.f1_score(y_deploy, y_pred_ASM, average=\"macro\"))\n",
    "print(\"F1 score of THOMPSON SAM.: \\t\", metrics.f1_score(y_deploy, y_pred_TS, average=\"macro\"))\n",
    "print(\"F1 score of HEDGE:         \\t\", metrics.f1_score(y_deploy, y_pred_HG, average=\"macro\"))\n",
    "print(\"F1 score of EXP3 :         \\t\", metrics.f1_score(y_deploy, y_pred_EXP, average=\"macro\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Plot the results Comparing to UCB1 three times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "accData = [metrics.accuracy_score(y_deploy, y_pred_neigh),\n",
    "           metrics.accuracy_score(y_deploy, y_pred_svm_linear), \n",
    "           metrics.accuracy_score(y_deploy, y_pred_svm), \n",
    "           metrics.accuracy_score(y_deploy, y_pred_gtgini),\n",
    "           metrics.accuracy_score(y_deploy, y_pred_dt_IG), \n",
    "           metrics.accuracy_score(y_deploy, y_pred_dt_BGKN),\n",
    "           metrics.accuracy_score(y_deploy, y_pred_dt_BGDT), \n",
    "           metrics.accuracy_score(y_deploy, y_pred_dt_RF),\n",
    "           metrics.accuracy_score(y_deploy, y_pred_dt_AD), \n",
    "           metrics.accuracy_score(y_deploy, y_pred_NB),\n",
    "           metrics.accuracy_score(y_deploy, y_pred_dt_LDA), \n",
    "           metrics.accuracy_score(y_deploy, y_pred_dt_QDA),\n",
    "           metrics.accuracy_score(y_deploy, y_pred_NN),\n",
    "           metrics.accuracy_score(y_deploy, y_pred_NN1),\n",
    "           metrics.accuracy_score(y_deploy, y_pred_NN2),\n",
    "           metrics.accuracy_score(y_deploy, y_pred_NN3),\n",
    "           metrics.accuracy_score(y_deploy, y_pred_NN4)]\n",
    "PresionData = [metrics.precision_score(y_deploy, y_pred_neigh, average=\"macro\"),\n",
    "               metrics.precision_score(y_deploy, y_pred_svm_linear, average=\"macro\"),\n",
    "               metrics.precision_score(y_deploy, y_pred_svm, average=\"macro\"),\n",
    "               metrics.precision_score(y_deploy, y_pred_gtgini, average=\"macro\"),\n",
    "               metrics.precision_score(y_deploy, y_pred_dt_IG, average=\"macro\"),\n",
    "               metrics.precision_score(y_deploy, y_pred_dt_BGKN, average=\"macro\"),\n",
    "               metrics.precision_score(y_deploy, y_pred_dt_BGDT, average=\"macro\"),\n",
    "               metrics.precision_score(y_deploy, y_pred_dt_RF, average=\"macro\"),\n",
    "               metrics.precision_score(y_deploy, y_pred_dt_AD, average=\"macro\"),\n",
    "               metrics.precision_score(y_deploy, y_pred_NB, average=\"macro\"),\n",
    "               metrics.precision_score(y_deploy, y_pred_dt_LDA, average=\"macro\"),\n",
    "               metrics.precision_score(y_deploy, y_pred_dt_QDA, average=\"macro\"),\n",
    "               metrics.precision_score(y_deploy, y_pred_NN, average=\"macro\"),\n",
    "               metrics.precision_score(y_deploy, y_pred_NN1, average=\"macro\"),\n",
    "               metrics.precision_score(y_deploy, y_pred_NN2, average=\"macro\"),\n",
    "               metrics.precision_score(y_deploy, y_pred_NN3, average=\"macro\"),\n",
    "               metrics.precision_score(y_deploy, y_pred_NN4, average=\"macro\")]\n",
    "RecallData = [ metrics.recall_score(y_deploy, y_pred_neigh, average=\"macro\"),\n",
    "              metrics.recall_score(y_deploy, y_pred_svm_linear, average=\"macro\"),\n",
    "              metrics.recall_score(y_deploy, y_pred_svm, average=\"macro\"),\n",
    "              metrics.recall_score(y_deploy, y_pred_gtgini, average=\"macro\"),\n",
    "              metrics.recall_score(y_deploy, y_pred_dt_IG, average=\"macro\"),\n",
    "              metrics.recall_score(y_deploy, y_pred_dt_BGKN, average=\"macro\"),\n",
    "              metrics.recall_score(y_deploy, y_pred_dt_BGDT, average=\"macro\"),\n",
    "              metrics.recall_score(y_deploy, y_pred_dt_RF, average=\"macro\"),\n",
    "              metrics.recall_score(y_deploy, y_pred_dt_AD, average=\"macro\"),\n",
    "              metrics.recall_score(y_deploy, y_pred_NB, average=\"macro\"),\n",
    "              metrics.recall_score(y_deploy, y_pred_dt_LDA, average=\"macro\"),\n",
    "              metrics.recall_score(y_deploy, y_pred_dt_QDA, average=\"macro\"),\n",
    "              metrics.recall_score(y_deploy, y_pred_NN, average=\"macro\"),\n",
    "              metrics.recall_score(y_deploy, y_pred_NN1, average=\"macro\"),\n",
    "              metrics.recall_score(y_deploy, y_pred_NN2, average=\"macro\"),\n",
    "              metrics.recall_score(y_deploy, y_pred_NN3, average=\"macro\"),\n",
    "              metrics.recall_score(y_deploy, y_pred_NN4, average=\"macro\")]\n",
    "F1Data = [metrics.f1_score(y_deploy, y_pred_neigh, average=\"macro\"),\n",
    "          metrics.f1_score(y_deploy, y_pred_svm_linear, average=\"macro\"),\n",
    "          metrics.f1_score(y_deploy, y_pred_svm, average=\"macro\"),\n",
    "          metrics.f1_score(y_deploy, y_pred_gtgini, average=\"macro\"),\n",
    "          metrics.f1_score(y_deploy, y_pred_dt_IG, average=\"macro\"),\n",
    "          metrics.f1_score(y_deploy, y_pred_dt_BGKN, average=\"macro\"),\n",
    "          metrics.f1_score(y_deploy, y_pred_dt_BGDT, average=\"macro\"),\n",
    "          metrics.f1_score(y_deploy, y_pred_dt_RF, average=\"macro\"),\n",
    "          metrics.f1_score(y_deploy, y_pred_dt_AD, average=\"macro\"),\n",
    "          metrics.f1_score(y_deploy, y_pred_NB, average=\"macro\"),\n",
    "          metrics.f1_score(y_deploy, y_pred_dt_LDA, average=\"macro\"),\n",
    "          metrics.f1_score(y_deploy, y_pred_dt_QDA, average=\"macro\"),\n",
    "          metrics.f1_score(y_deploy, y_pred_NN, average=\"macro\"),\n",
    "          metrics.f1_score(y_deploy, y_pred_NN1, average=\"macro\"),\n",
    "          metrics.f1_score(y_deploy, y_pred_NN2, average=\"macro\"),\n",
    "          metrics.f1_score(y_deploy, y_pred_NN3, average=\"macro\"),\n",
    "          metrics.f1_score(y_deploy, y_pred_NN4, average=\"macro\")]\n",
    "N = len(accData)\n",
    "## necessary variables\n",
    "ind = np.arange(N)                # the x locations for the groups\n",
    "width = 0.15                     # the width of the bars\n",
    "## the bars\n",
    "rects1 = ax.bar(ind, accData, width,\n",
    "                color='black',\n",
    "                #yerr=menStd,\n",
    "                error_kw=dict(elinewidth=2,ecolor='red'))\n",
    "rects2 = ax.bar(ind+width, F1Data, width,\n",
    "                    color='red',\n",
    "                    #yerr=womenStd,\n",
    "                    error_kw=dict(elinewidth=2,ecolor='black'))\n",
    "rects3 = ax.bar(ind+width+width, PresionData, width,\n",
    "                    color='green',\n",
    "                    #yerr=womenStd,\n",
    "                    error_kw=dict(elinewidth=2,ecolor='blue'))\n",
    "rects4 = ax.bar(ind+width+width+width, RecallData, width,\n",
    "                    color='blue',\n",
    "                    #yerr=womenStd,\n",
    "                    error_kw=dict(elinewidth=2,ecolor='green'))\n",
    "# axes and labels\n",
    "ax.set_xlim(-width,len(ind)+width)\n",
    "ax.set_ylim(0,1)\n",
    "ax.set_ylabel('Score')\n",
    "ax.set_title('Scores of different classifiers on Test Data')\n",
    "xTickMarks = ['Knn', 'LSVM', 'SVM', 'DT_gini', 'DT_entorpy' ,\n",
    "              'Bagging Knn' , 'Bagging DT' , 'Random Forest' , 'Ada Boost' ,\n",
    "              'NB' , 'LDA' , 'QDA' , 'NN', 'NN2.5%R' , 'NN7.5%R' , 'NN65%R' , 'NN85%R']\n",
    "ax.set_xticks(ind+width)\n",
    "xtickNames = ax.set_xticklabels(xTickMarks)\n",
    "plt.setp(xtickNames, rotation=45, fontsize=10)\n",
    "\n",
    "\n",
    "\n",
    "## add a legend\n",
    "ax.legend( (rects1[0], rects2[0], rects3[0], rects4[0]), ('Acc.', 'F1' , 'Prec.' , 'Recall') , loc=8, fancybox=True, \n",
    "          frameon=True, shadow=True)\n",
    "ax.set_facecolor('0.9')\n",
    "\n",
    "ax.spines['top'].set_visible(True)\n",
    "ax.spines['right'].set_visible(True)\n",
    "ax.spines['bottom'].set_visible(True)\n",
    "ax.spines['left'].set_visible(True)\n",
    "\n",
    "ax.spines['top'].set_linewidth(0.9)\n",
    "ax.spines['right'].set_linewidth(0.9)\n",
    "ax.spines['bottom'].set_linewidth(0.9)\n",
    "ax.spines['left'].set_linewidth(0.9)\n",
    "ax.grid(False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from math import pi\n",
    "from bokeh.charts import Bar, Area, defaults\n",
    "from bokeh.layouts import row\n",
    "from bokeh.charts.attributes import cat, color\n",
    "from bokeh.charts.operations import blend\n",
    "#from bokeh.charts.utils import df_from_json\n",
    "from bokeh.plotting import figure, output_notebook, show\n",
    "############################################################################################################\n",
    "TOOLS = 'box_zoom,box_select,crosshair,resize,reset,lasso_select,pan,save,poly_select,tap,wheel_zoom,undo'\n",
    "#defaults.width = 1000\n",
    "#defaults.height = 800\n",
    "output_notebook()\n",
    "df1 = pd.DataFrame({'Matric': xTickMarks,\n",
    "                    'Accuracy':accData, \n",
    "                   'Precision': PresionData, \n",
    "                   'Recall': RecallData, \n",
    "                    'F1 Score': F1Data})\n",
    "############################################################################################################\n",
    "bar = Bar(df1,\n",
    "          values=blend('Accuracy', 'F1 Score', 'Precision','Recall', name='Scores', labels_name='Score'),\n",
    "          label=cat(columns='Matric', sort=False),\n",
    "          stack=cat(columns='Score', sort=False),\n",
    "          color=color(columns='Score', palette=['SaddleBrown', 'Silver', 'Goldenrod', 'Grey'],\n",
    "                      sort=False),\n",
    "          legend='bottom_center', xlabel=\"List of Models\", ylabel=\"The Scores\",\n",
    "          title=\"Scores of different Models\", \n",
    "          tooltips=[('Score', '@Score'), ('Model', '@Matric')],\n",
    "          tools=TOOLS, plot_width=900, plot_height=800)\n",
    "bar.title.align = \"center\"\n",
    "bar.xaxis.major_label_orientation = pi/2\n",
    "###############################################################################################################\n",
    "p = Bar(df1, label='Matric', \n",
    "        values = blend('Accuracy', 'F1 Score', 'Precision','Recall', name='Scores', labels_name='Score'),\n",
    "        group=cat(columns='Score', sort=False),\n",
    "        title=\"Scores of different Models\", legend='bottom_center',\n",
    "       tools=TOOLS, plot_width=900, plot_height=600,\n",
    "       xlabel='List of Models', ylabel='The Scores')\n",
    "p.title.align = \"center\"\n",
    "#p.yaxis.major_label_orientation = \"vertical\"\n",
    "p.xaxis.major_label_orientation = pi/2\n",
    "#########################################################################################################\n",
    "data = dict(\n",
    "    Acc = accData,\n",
    "    Pre = PresionData,\n",
    "    Rec = RecallData,\n",
    "    F1 = F1Data,\n",
    ")\n",
    "area1 = Area(data, title=\"The trend of score over Models\", legend=\"bottom_center\",\n",
    "             xlabel='List of Models', ylabel='The Scores',\n",
    "            tools=TOOLS, plot_width=450, plot_height=300)\n",
    "area1.title.align = \"center\"\n",
    "area2 = Area(data, title=\"The trend of score over Models\", legend=\"bottom_center\",\n",
    "             stack=True, xlabel='List of Models', ylabel='The Scores',\n",
    "            tools=TOOLS, plot_width=450, plot_height=300)\n",
    "area2.title.align = \"center\"\n",
    "#########################################################################################################\n",
    "show(bar)\n",
    "show(p)\n",
    "#show(area1)\n",
    "#show(area2)\n",
    "show(row(area1, area2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compare All the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "accData = [metrics.accuracy_score(y_deploy, y_pred_neigh),\n",
    "           metrics.accuracy_score(y_deploy, y_pred_svm_linear), \n",
    "           metrics.accuracy_score(y_deploy, y_pred_svm), \n",
    "           metrics.accuracy_score(y_deploy, y_pred_gtgini),\n",
    "           metrics.accuracy_score(y_deploy, y_pred_dt_IG), \n",
    "           metrics.accuracy_score(y_deploy, y_pred_dt_BGKN),\n",
    "           metrics.accuracy_score(y_deploy, y_pred_dt_BGDT), \n",
    "           metrics.accuracy_score(y_deploy, y_pred_dt_RF),\n",
    "           metrics.accuracy_score(y_deploy, y_pred_dt_AD), \n",
    "           metrics.accuracy_score(y_deploy, y_pred_NB),\n",
    "           metrics.accuracy_score(y_deploy, y_pred_dt_LDA), \n",
    "           metrics.accuracy_score(y_deploy, y_pred_dt_QDA),           \n",
    "           metrics.accuracy_score(y_deploy, y_pred_LR),\n",
    "           metrics.accuracy_score(y_deploy, y_pred_GP),\n",
    "           metrics.accuracy_score(y_deploy, y_pred_gbm),\n",
    "           metrics.accuracy_score(y_deploy, y_pred_xgboost),\n",
    "           metrics.accuracy_score(y_deploy, y_pred_NN),        \n",
    "           metrics.accuracy_score(y_deploy, y_pred_NN1),\n",
    "           metrics.accuracy_score(y_deploy, y_pred_EG),\n",
    "           metrics.accuracy_score(y_deploy, y_pred_AEG),\n",
    "           metrics.accuracy_score(y_deploy, y_pred_SM),\n",
    "           metrics.accuracy_score(y_deploy, y_pred_ASM),\n",
    "           metrics.accuracy_score(y_deploy, y_pred_TS),\n",
    "           metrics.accuracy_score(y_deploy, y_pred_HG),\n",
    "           metrics.accuracy_score(y_deploy, y_pred_EXP)]\n",
    "          \n",
    "PresionData = [metrics.precision_score(y_deploy, y_pred_neigh, average=\"macro\"),\n",
    "               metrics.precision_score(y_deploy, y_pred_svm_linear, average=\"macro\"),\n",
    "               metrics.precision_score(y_deploy, y_pred_svm, average=\"macro\"),\n",
    "               metrics.precision_score(y_deploy, y_pred_gtgini, average=\"macro\"),\n",
    "               metrics.precision_score(y_deploy, y_pred_dt_IG, average=\"macro\"),\n",
    "               metrics.precision_score(y_deploy, y_pred_dt_BGKN, average=\"macro\"),\n",
    "               metrics.precision_score(y_deploy, y_pred_dt_BGDT, average=\"macro\"),\n",
    "               metrics.precision_score(y_deploy, y_pred_dt_RF, average=\"macro\"),\n",
    "               metrics.precision_score(y_deploy, y_pred_dt_AD, average=\"macro\"),\n",
    "               metrics.precision_score(y_deploy, y_pred_NB, average=\"macro\"),\n",
    "               metrics.precision_score(y_deploy, y_pred_dt_LDA, average=\"macro\"),\n",
    "               metrics.precision_score(y_deploy, y_pred_dt_QDA, average=\"macro\"),              \n",
    "               metrics.precision_score(y_deploy, y_pred_LR, average=\"macro\"),\n",
    "               metrics.precision_score(y_deploy, y_pred_GP, average=\"macro\"),\n",
    "               metrics.precision_score(y_deploy, y_pred_gbm, average=\"macro\"),\n",
    "               metrics.precision_score(y_deploy, y_pred_xgboost, average=\"macro\"),\n",
    "               metrics.precision_score(y_deploy, y_pred_NN, average=\"macro\"),\n",
    "               metrics.precision_score(y_deploy, y_pred_NN1, average=\"macro\"),\n",
    "               metrics.precision_score(y_deploy, y_pred_EG, average=\"macro\"),\n",
    "               metrics.precision_score(y_deploy, y_pred_AEG, average=\"macro\"),\n",
    "               metrics.precision_score(y_deploy, y_pred_SM, average=\"macro\"),\n",
    "               metrics.precision_score(y_deploy, y_pred_ASM, average=\"macro\"),\n",
    "               metrics.precision_score(y_deploy, y_pred_TS, average=\"macro\"),\n",
    "               metrics.precision_score(y_deploy, y_pred_HG, average=\"macro\"),\n",
    "               metrics.precision_score(y_deploy, y_pred_EXP, average=\"macro\")]\n",
    "RecallData = [ metrics.recall_score(y_deploy, y_pred_neigh, average=\"macro\"),\n",
    "              metrics.recall_score(y_deploy, y_pred_svm_linear, average=\"macro\"),\n",
    "              metrics.recall_score(y_deploy, y_pred_svm, average=\"macro\"),\n",
    "              metrics.recall_score(y_deploy, y_pred_gtgini, average=\"macro\"),\n",
    "              metrics.recall_score(y_deploy, y_pred_dt_IG, average=\"macro\"),\n",
    "              metrics.recall_score(y_deploy, y_pred_dt_BGKN, average=\"macro\"),\n",
    "              metrics.recall_score(y_deploy, y_pred_dt_BGDT, average=\"macro\"),\n",
    "              metrics.recall_score(y_deploy, y_pred_dt_RF, average=\"macro\"),\n",
    "              metrics.recall_score(y_deploy, y_pred_dt_AD, average=\"macro\"),\n",
    "              metrics.recall_score(y_deploy, y_pred_NB, average=\"macro\"),\n",
    "              metrics.recall_score(y_deploy, y_pred_dt_LDA, average=\"macro\"),\n",
    "              metrics.recall_score(y_deploy, y_pred_dt_QDA, average=\"macro\"),\n",
    "              metrics.recall_score(y_deploy, y_pred_LR, average=\"macro\"),\n",
    "              metrics.recall_score(y_deploy, y_pred_GP, average=\"macro\"),\n",
    "              metrics.recall_score(y_deploy, y_pred_gbm, average=\"macro\"),\n",
    "              metrics.recall_score(y_deploy, y_pred_xgboost, average=\"macro\"),       \n",
    "              metrics.recall_score(y_deploy, y_pred_NN, average=\"macro\"),\n",
    "              metrics.recall_score(y_deploy, y_pred_NN1, average=\"macro\"),\n",
    "              metrics.recall_score(y_deploy, y_pred_EG, average=\"macro\"),\n",
    "              metrics.recall_score(y_deploy, y_pred_AEG, average=\"macro\"),\n",
    "              metrics.recall_score(y_deploy, y_pred_SM, average=\"macro\"),\n",
    "              metrics.recall_score(y_deploy, y_pred_ASM, average=\"macro\"),\n",
    "              metrics.recall_score(y_deploy, y_pred_TS, average=\"macro\"),\n",
    "              metrics.recall_score(y_deploy, y_pred_HG, average=\"macro\"),\n",
    "              metrics.recall_score(y_deploy, y_pred_EXP, average=\"macro\")]\n",
    "F1Data = [metrics.f1_score(y_deploy, y_pred_neigh, average=\"macro\"),\n",
    "          metrics.f1_score(y_deploy, y_pred_svm_linear, average=\"macro\"),\n",
    "          metrics.f1_score(y_deploy, y_pred_svm, average=\"macro\"),\n",
    "          metrics.f1_score(y_deploy, y_pred_gtgini, average=\"macro\"),\n",
    "          metrics.f1_score(y_deploy, y_pred_dt_IG, average=\"macro\"),\n",
    "          metrics.f1_score(y_deploy, y_pred_dt_BGKN, average=\"macro\"),\n",
    "          metrics.f1_score(y_deploy, y_pred_dt_BGDT, average=\"macro\"),\n",
    "          metrics.f1_score(y_deploy, y_pred_dt_RF, average=\"macro\"),\n",
    "          metrics.f1_score(y_deploy, y_pred_dt_AD, average=\"macro\"),\n",
    "          metrics.f1_score(y_deploy, y_pred_NB, average=\"macro\"),\n",
    "          metrics.f1_score(y_deploy, y_pred_dt_LDA, average=\"macro\"),\n",
    "          metrics.f1_score(y_deploy, y_pred_dt_QDA, average=\"macro\"),         \n",
    "          metrics.f1_score(y_deploy, y_pred_LR, average=\"macro\"),\n",
    "          metrics.f1_score(y_deploy, y_pred_GP, average=\"macro\"),\n",
    "          metrics.f1_score(y_deploy, y_pred_gbm, average=\"macro\"),\n",
    "          metrics.f1_score(y_deploy, y_pred_xgboost, average=\"macro\"),\n",
    "          metrics.f1_score(y_deploy, y_pred_NN, average=\"macro\"),                             \n",
    "          metrics.f1_score(y_deploy, y_pred_NN1, average=\"macro\"),\n",
    "          metrics.f1_score(y_deploy, y_pred_EG, average=\"macro\"),\n",
    "          metrics.f1_score(y_deploy, y_pred_AEG, average=\"macro\"),\n",
    "          metrics.f1_score(y_deploy, y_pred_SM, average=\"macro\"),\n",
    "          metrics.f1_score(y_deploy, y_pred_ASM, average=\"macro\"),\n",
    "          metrics.f1_score(y_deploy, y_pred_TS, average=\"macro\"),\n",
    "          metrics.f1_score(y_deploy, y_pred_HG, average=\"macro\"),\n",
    "          metrics.f1_score(y_deploy, y_pred_EXP, average=\"macro\")]         \n",
    "\n",
    "\n",
    "\n",
    "N = len(accData)\n",
    "## necessary variables\n",
    "ind = np.arange(N)                # the x locations for the groups\n",
    "width = 0.17                     # the width of the bars\n",
    "## the bars\n",
    "rects1 = ax.bar(ind, accData, width,\n",
    "                color='black',\n",
    "                #yerr=menStd,\n",
    "                error_kw=dict(elinewidth=2,ecolor='red'))\n",
    "rects2 = ax.bar(ind+width, F1Data, width,\n",
    "                    color='red',\n",
    "                    #yerr=womenStd,\n",
    "                    error_kw=dict(elinewidth=2,ecolor='black'))\n",
    "rects3 = ax.bar(ind+width+width, PresionData, width,\n",
    "                    color='green',\n",
    "                    #yerr=womenStd,\n",
    "                    error_kw=dict(elinewidth=2,ecolor='blue'))\n",
    "rects4 = ax.bar(ind+width+width+width, RecallData, width,\n",
    "                    color='blue',\n",
    "                    #yerr=womenStd,\n",
    "                    error_kw=dict(elinewidth=2,ecolor='green'))\n",
    "# axes and labels\n",
    "ax.set_xlim(-width,len(ind)+width)\n",
    "ax.set_ylim(0,1)\n",
    "ax.set_ylabel('Score')\n",
    "ax.set_xlabel('Name of Classifier')\n",
    "ax.set_title('Scores of different classifiers on Test Data')\n",
    "xTickMarks = ['Knn', 'LSVM', 'SVM', 'DT_gini', 'DT_entorpy' ,\n",
    "              'Bagging Knn' , 'Bagging DT' , 'Random Forest' , 'Ada Boost' ,\n",
    "              'NB' , 'LDA' , 'QDA' ,'Log. Reg.' ,'GP Class.','LightGBM','Xgboost',\n",
    "              'NN', 'UCB1' , 'E Greedy' , 'Decay E Gr.' , 'Softmax', 'Decay SM',\n",
    "               'Tomp. Sampling', 'Hedge', 'EXP3']\n",
    "ax.set_xticks(ind+width)\n",
    "xtickNames = ax.set_xticklabels(xTickMarks)\n",
    "plt.setp(xtickNames, rotation=90, fontsize=10)\n",
    "## add a legend\n",
    "ax.legend( (rects1[0], rects2[0], rects3[0], rects4[0]), ('Acc.', 'F1' , 'Prec.' , 'Recall') , loc=8, fancybox=True, \n",
    "          frameon=True, shadow=True)\n",
    "ax.set_facecolor('0.9')\n",
    "\n",
    "ax.spines['top'].set_visible(True)\n",
    "ax.spines['right'].set_visible(True)\n",
    "ax.spines['bottom'].set_visible(True)\n",
    "ax.spines['left'].set_visible(True)\n",
    "\n",
    "ax.spines['top'].set_linewidth(0.9)\n",
    "ax.spines['right'].set_linewidth(0.9)\n",
    "ax.spines['bottom'].set_linewidth(0.9)\n",
    "ax.spines['left'].set_linewidth(0.9)\n",
    "ax.grid(False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from math import pi\n",
    "from bokeh.charts import Bar, Area, defaults\n",
    "from bokeh.layouts import row\n",
    "from bokeh.charts.attributes import cat, color\n",
    "from bokeh.charts.operations import blend\n",
    "#from bokeh.charts.utils import df_from_json\n",
    "from bokeh.plotting import figure, output_notebook, show\n",
    "############################################################################################################\n",
    "TOOLS = 'box_zoom,box_select,crosshair,resize,reset,lasso_select,pan,save,poly_select,tap,wheel_zoom,undo'\n",
    "#defaults.width = 1000\n",
    "#defaults.height = 800\n",
    "output_notebook()\n",
    "df1 = pd.DataFrame({'Matric': xTickMarks,\n",
    "                    'Accuracy':accData, \n",
    "                   'Precision': PresionData, \n",
    "                   'Recall': RecallData, \n",
    "                    'F1 Score': F1Data})\n",
    "############################################################################################################\n",
    "bar = Bar(df1,\n",
    "          values=blend('Accuracy', 'F1 Score', 'Precision','Recall', name='Scores', labels_name='Score'),\n",
    "          label=cat(columns='Matric', sort=False),\n",
    "          stack=cat(columns='Score', sort=False),\n",
    "          color=color(columns='Score', palette=['SaddleBrown', 'Silver', 'Goldenrod', 'Grey'],\n",
    "                      sort=False),\n",
    "          legend='bottom_center', xlabel=\"List of Models\", ylabel=\"The Scores\",\n",
    "          title=\"Scores of different Models\", \n",
    "          tooltips=[('Score', '@Score'), ('Model', '@Matric')],\n",
    "          tools=TOOLS, plot_width=900, plot_height=800)\n",
    "bar.title.align = \"center\"\n",
    "bar.xaxis.major_label_orientation = pi/2\n",
    "###############################################################################################################\n",
    "p = Bar(df1, label='Matric', \n",
    "        values = blend('Accuracy', 'F1 Score', 'Precision','Recall', name='Scores', labels_name='Score'),\n",
    "        group=cat(columns='Score', sort=False),\n",
    "        title=\"Scores of different Models\", legend='bottom_center',\n",
    "       tools=TOOLS, plot_width=900, plot_height=600,\n",
    "       xlabel='List of Models', ylabel='The Scores')\n",
    "p.title.align = \"center\"\n",
    "#p.yaxis.major_label_orientation = \"vertical\"\n",
    "p.xaxis.major_label_orientation = pi/2\n",
    "#########################################################################################################\n",
    "data = dict(\n",
    "    Acc = accData,\n",
    "    Pre = PresionData,\n",
    "    Rec = RecallData,\n",
    "    F1 = F1Data,\n",
    ")\n",
    "area1 = Area(data, title=\"The trend of score over Models\", legend=\"bottom_center\",\n",
    "             xlabel='List of Models', ylabel='The Scores',\n",
    "            tools=TOOLS, plot_width=450, plot_height=300)\n",
    "area1.title.align = \"center\"\n",
    "area2 = Area(data, title=\"The trend of score over Models\", legend=\"bottom_center\",\n",
    "             stack=True, xlabel='List of Models', ylabel='The Scores',\n",
    "            tools=TOOLS, plot_width=450, plot_height=300)\n",
    "area2.title.align = \"center\"\n",
    "#########################################################################################################\n",
    "show(bar)\n",
    "show(p)\n",
    "#show(area1)\n",
    "#show(area2)\n",
    "show(row(area1, area2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compare UCB1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "accData = [metrics.accuracy_score(y_deploy, y_pred_neigh),\n",
    "           metrics.accuracy_score(y_deploy, y_pred_svm_linear), \n",
    "           metrics.accuracy_score(y_deploy, y_pred_svm), \n",
    "           metrics.accuracy_score(y_deploy, y_pred_gtgini),\n",
    "           metrics.accuracy_score(y_deploy, y_pred_dt_IG), \n",
    "           metrics.accuracy_score(y_deploy, y_pred_dt_BGKN),\n",
    "           metrics.accuracy_score(y_deploy, y_pred_dt_BGDT), \n",
    "           metrics.accuracy_score(y_deploy, y_pred_dt_RF),\n",
    "           metrics.accuracy_score(y_deploy, y_pred_dt_AD), \n",
    "           metrics.accuracy_score(y_deploy, y_pred_NB),\n",
    "           metrics.accuracy_score(y_deploy, y_pred_dt_LDA), \n",
    "           metrics.accuracy_score(y_deploy, y_pred_dt_QDA),           \n",
    "           metrics.accuracy_score(y_deploy, y_pred_LR),\n",
    "           metrics.accuracy_score(y_deploy, y_pred_GP),\n",
    "           metrics.accuracy_score(y_deploy, y_pred_gbm),\n",
    "           metrics.accuracy_score(y_deploy, y_pred_xgboost),\n",
    "           metrics.accuracy_score(y_deploy, y_pred_NN),        \n",
    "           metrics.accuracy_score(y_deploy, y_pred_NN1)]\n",
    "          \n",
    "PresionData = [metrics.precision_score(y_deploy, y_pred_neigh, average=\"macro\"),\n",
    "               metrics.precision_score(y_deploy, y_pred_svm_linear, average=\"macro\"),\n",
    "               metrics.precision_score(y_deploy, y_pred_svm, average=\"macro\"),\n",
    "               metrics.precision_score(y_deploy, y_pred_gtgini, average=\"macro\"),\n",
    "               metrics.precision_score(y_deploy, y_pred_dt_IG, average=\"macro\"),\n",
    "               metrics.precision_score(y_deploy, y_pred_dt_BGKN, average=\"macro\"),\n",
    "               metrics.precision_score(y_deploy, y_pred_dt_BGDT, average=\"macro\"),\n",
    "               metrics.precision_score(y_deploy, y_pred_dt_RF, average=\"macro\"),\n",
    "               metrics.precision_score(y_deploy, y_pred_dt_AD, average=\"macro\"),\n",
    "               metrics.precision_score(y_deploy, y_pred_NB, average=\"macro\"),\n",
    "               metrics.precision_score(y_deploy, y_pred_dt_LDA, average=\"macro\"),\n",
    "               metrics.precision_score(y_deploy, y_pred_dt_QDA, average=\"macro\"),              \n",
    "               metrics.precision_score(y_deploy, y_pred_LR, average=\"macro\"),\n",
    "               metrics.precision_score(y_deploy, y_pred_GP, average=\"macro\"),\n",
    "               metrics.precision_score(y_deploy, y_pred_gbm, average=\"macro\"),\n",
    "               metrics.precision_score(y_deploy, y_pred_xgboost, average=\"macro\"),\n",
    "               metrics.precision_score(y_deploy, y_pred_NN, average=\"macro\"),\n",
    "               metrics.precision_score(y_deploy, y_pred_NN1, average=\"macro\")]\n",
    "\n",
    "RecallData = [ metrics.recall_score(y_deploy, y_pred_neigh, average=\"macro\"),\n",
    "              metrics.recall_score(y_deploy, y_pred_svm_linear, average=\"macro\"),\n",
    "              metrics.recall_score(y_deploy, y_pred_svm, average=\"macro\"),\n",
    "              metrics.recall_score(y_deploy, y_pred_gtgini, average=\"macro\"),\n",
    "              metrics.recall_score(y_deploy, y_pred_dt_IG, average=\"macro\"),\n",
    "              metrics.recall_score(y_deploy, y_pred_dt_BGKN, average=\"macro\"),\n",
    "              metrics.recall_score(y_deploy, y_pred_dt_BGDT, average=\"macro\"),\n",
    "              metrics.recall_score(y_deploy, y_pred_dt_RF, average=\"macro\"),\n",
    "              metrics.recall_score(y_deploy, y_pred_dt_AD, average=\"macro\"),\n",
    "              metrics.recall_score(y_deploy, y_pred_NB, average=\"macro\"),\n",
    "              metrics.recall_score(y_deploy, y_pred_dt_LDA, average=\"macro\"),\n",
    "              metrics.recall_score(y_deploy, y_pred_dt_QDA, average=\"macro\"),\n",
    "              metrics.recall_score(y_deploy, y_pred_LR, average=\"macro\"),\n",
    "              metrics.recall_score(y_deploy, y_pred_GP, average=\"macro\"),\n",
    "              metrics.recall_score(y_deploy, y_pred_gbm, average=\"macro\"),\n",
    "              metrics.recall_score(y_deploy, y_pred_xgboost, average=\"macro\"),       \n",
    "              metrics.recall_score(y_deploy, y_pred_NN, average=\"macro\"),\n",
    "              metrics.recall_score(y_deploy, y_pred_NN1, average=\"macro\")]\n",
    "\n",
    "F1Data = [metrics.f1_score(y_deploy, y_pred_neigh, average=\"macro\"),\n",
    "          metrics.f1_score(y_deploy, y_pred_svm_linear, average=\"macro\"),\n",
    "          metrics.f1_score(y_deploy, y_pred_svm, average=\"macro\"),\n",
    "          metrics.f1_score(y_deploy, y_pred_gtgini, average=\"macro\"),\n",
    "          metrics.f1_score(y_deploy, y_pred_dt_IG, average=\"macro\"),\n",
    "          metrics.f1_score(y_deploy, y_pred_dt_BGKN, average=\"macro\"),\n",
    "          metrics.f1_score(y_deploy, y_pred_dt_BGDT, average=\"macro\"),\n",
    "          metrics.f1_score(y_deploy, y_pred_dt_RF, average=\"macro\"),\n",
    "          metrics.f1_score(y_deploy, y_pred_dt_AD, average=\"macro\"),\n",
    "          metrics.f1_score(y_deploy, y_pred_NB, average=\"macro\"),\n",
    "          metrics.f1_score(y_deploy, y_pred_dt_LDA, average=\"macro\"),\n",
    "          metrics.f1_score(y_deploy, y_pred_dt_QDA, average=\"macro\"),         \n",
    "          metrics.f1_score(y_deploy, y_pred_LR, average=\"macro\"),\n",
    "          metrics.f1_score(y_deploy, y_pred_GP, average=\"macro\"),\n",
    "          metrics.f1_score(y_deploy, y_pred_gbm, average=\"macro\"),\n",
    "          metrics.f1_score(y_deploy, y_pred_xgboost, average=\"macro\"),\n",
    "          metrics.f1_score(y_deploy, y_pred_NN, average=\"macro\"),                             \n",
    "          metrics.f1_score(y_deploy, y_pred_NN1, average=\"macro\")]         \n",
    "\n",
    "\n",
    "\n",
    "N = len(accData)\n",
    "## necessary variables\n",
    "ind = np.arange(N)                # the x locations for the groups\n",
    "width = 0.17                     # the width of the bars\n",
    "## the bars\n",
    "rects1 = ax.bar(ind, accData, width,\n",
    "                color='black',\n",
    "                #yerr=menStd,\n",
    "                error_kw=dict(elinewidth=2,ecolor='red'))\n",
    "rects2 = ax.bar(ind+width, F1Data, width,\n",
    "                    color='red',\n",
    "                    #yerr=womenStd,\n",
    "                    error_kw=dict(elinewidth=2,ecolor='black'))\n",
    "rects3 = ax.bar(ind+width+width, PresionData, width,\n",
    "                    color='green',\n",
    "                    #yerr=womenStd,\n",
    "                    error_kw=dict(elinewidth=2,ecolor='blue'))\n",
    "rects4 = ax.bar(ind+width+width+width, RecallData, width,\n",
    "                    color='blue',\n",
    "                    #yerr=womenStd,\n",
    "                    error_kw=dict(elinewidth=2,ecolor='green'))\n",
    "# axes and labels\n",
    "ax.set_xlim(-width,len(ind)+width)\n",
    "ax.set_ylim(0,1)\n",
    "ax.set_ylabel('Score')\n",
    "ax.set_xlabel('Name of Classifier')\n",
    "ax.set_title('Scores of different classifiers on Test Data')\n",
    "xTickMarks = ['Knn', 'LSVM', 'SVM', 'DT_gini', 'DT_entorpy' ,\n",
    "              'Bagging Knn' , 'Bagging DT' , 'Random Forest' , 'Ada Boost' ,\n",
    "              'NB' , 'LDA' , 'QDA' ,'Log. Reg.' ,'GP Class.','LightGBM','Xgboost',\n",
    "              'NN', 'UCB1']\n",
    "ax.set_xticks(ind+width)\n",
    "xtickNames = ax.set_xticklabels(xTickMarks)\n",
    "plt.setp(xtickNames, rotation=90, fontsize=10)\n",
    "## add a legend\n",
    "ax.legend( (rects1[0], rects2[0], rects3[0], rects4[0]), ('Acc.', 'F1' , 'Prec.' , 'Recall') , loc=8, fancybox=True, \n",
    "          frameon=True, shadow=True)\n",
    "ax.set_facecolor('0.9')\n",
    "\n",
    "ax.spines['top'].set_visible(True)\n",
    "ax.spines['right'].set_visible(True)\n",
    "ax.spines['bottom'].set_visible(True)\n",
    "ax.spines['left'].set_visible(True)\n",
    "\n",
    "ax.spines['top'].set_linewidth(0.9)\n",
    "ax.spines['right'].set_linewidth(0.9)\n",
    "ax.spines['bottom'].set_linewidth(0.9)\n",
    "ax.spines['left'].set_linewidth(0.9)\n",
    "ax.grid(False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from math import pi\n",
    "from bokeh.charts import Bar, Area, defaults\n",
    "from bokeh.layouts import row\n",
    "from bokeh.charts.attributes import cat, color\n",
    "from bokeh.charts.operations import blend\n",
    "#from bokeh.charts.utils import df_from_json\n",
    "from bokeh.plotting import figure, output_notebook, show\n",
    "############################################################################################################\n",
    "TOOLS = 'box_zoom,box_select,crosshair,resize,reset,lasso_select,pan,save,poly_select,tap,wheel_zoom,undo'\n",
    "#defaults.width = 1000\n",
    "#defaults.height = 800\n",
    "output_notebook()\n",
    "df1 = pd.DataFrame({'Matric': xTickMarks,\n",
    "                    'Accuracy':accData, \n",
    "                   'Precision': PresionData, \n",
    "                   'Recall': RecallData, \n",
    "                    'F1 Score': F1Data})\n",
    "############################################################################################################\n",
    "bar = Bar(df1,\n",
    "          values=blend('Accuracy', 'F1 Score', 'Precision','Recall', name='Scores', labels_name='Score'),\n",
    "          label=cat(columns='Matric', sort=False),\n",
    "          stack=cat(columns='Score', sort=False),\n",
    "          color=color(columns='Score', palette=['SaddleBrown', 'Silver', 'Goldenrod', 'Grey'],\n",
    "                      sort=False),\n",
    "          legend='bottom_center', xlabel=\"List of Models\", ylabel=\"The Scores\",\n",
    "          title=\"Scores of different Models\", \n",
    "          tooltips=[('Score', '@Score'), ('Model', '@Matric')],\n",
    "          tools=TOOLS, plot_width=900, plot_height=800)\n",
    "bar.title.align = \"center\"\n",
    "bar.xaxis.major_label_orientation = pi/2\n",
    "###############################################################################################################\n",
    "p = Bar(df1, label='Matric', \n",
    "        values = blend('Accuracy', 'F1 Score', 'Precision','Recall', name='Scores', labels_name='Score'),\n",
    "        group=cat(columns='Score', sort=False),\n",
    "        title=\"Scores of different Models\", legend='bottom_center',\n",
    "       tools=TOOLS, plot_width=900, plot_height=600,\n",
    "       xlabel='List of Models', ylabel='The Scores')\n",
    "p.title.align = \"center\"\n",
    "#p.yaxis.major_label_orientation = \"vertical\"\n",
    "p.xaxis.major_label_orientation = pi/2\n",
    "#########################################################################################################\n",
    "data = dict(\n",
    "    Acc = accData,\n",
    "    Pre = PresionData,\n",
    "    Rec = RecallData,\n",
    "    F1 = F1Data,\n",
    ")\n",
    "area1 = Area(data, title=\"The trend of score over Models\", legend=\"bottom_center\",\n",
    "             xlabel='List of Models', ylabel='The Scores',\n",
    "            tools=TOOLS, plot_width=450, plot_height=300)\n",
    "area1.title.align = \"center\"\n",
    "area2 = Area(data, title=\"The trend of score over Models\", legend=\"bottom_center\",\n",
    "             stack=True, xlabel='List of Models', ylabel='The Scores',\n",
    "            tools=TOOLS, plot_width=450, plot_height=300)\n",
    "area2.title.align = \"center\"\n",
    "#########################################################################################################\n",
    "show(bar)\n",
    "show(p)\n",
    "#show(area1)\n",
    "#show(area2)\n",
    "show(row(area1, area2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compare Epsilon greedy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "accData = [metrics.accuracy_score(y_deploy, y_pred_neigh),\n",
    "           metrics.accuracy_score(y_deploy, y_pred_svm_linear), \n",
    "           metrics.accuracy_score(y_deploy, y_pred_svm), \n",
    "           metrics.accuracy_score(y_deploy, y_pred_gtgini),\n",
    "           metrics.accuracy_score(y_deploy, y_pred_dt_IG), \n",
    "           metrics.accuracy_score(y_deploy, y_pred_dt_BGKN),\n",
    "           metrics.accuracy_score(y_deploy, y_pred_dt_BGDT), \n",
    "           metrics.accuracy_score(y_deploy, y_pred_dt_RF),\n",
    "           metrics.accuracy_score(y_deploy, y_pred_dt_AD), \n",
    "           metrics.accuracy_score(y_deploy, y_pred_NB),\n",
    "           metrics.accuracy_score(y_deploy, y_pred_dt_LDA), \n",
    "           metrics.accuracy_score(y_deploy, y_pred_dt_QDA),           \n",
    "           metrics.accuracy_score(y_deploy, y_pred_LR),\n",
    "           metrics.accuracy_score(y_deploy, y_pred_GP),\n",
    "           metrics.accuracy_score(y_deploy, y_pred_gbm),\n",
    "           metrics.accuracy_score(y_deploy, y_pred_xgboost),\n",
    "           metrics.accuracy_score(y_deploy, y_pred_NN),        \n",
    "           metrics.accuracy_score(y_deploy, y_pred_EG),\n",
    "           metrics.accuracy_score(y_deploy, y_pred_AEG)]\n",
    "          \n",
    "PresionData = [metrics.precision_score(y_deploy, y_pred_neigh, average=\"macro\"),\n",
    "               metrics.precision_score(y_deploy, y_pred_svm_linear, average=\"macro\"),\n",
    "               metrics.precision_score(y_deploy, y_pred_svm, average=\"macro\"),\n",
    "               metrics.precision_score(y_deploy, y_pred_gtgini, average=\"macro\"),\n",
    "               metrics.precision_score(y_deploy, y_pred_dt_IG, average=\"macro\"),\n",
    "               metrics.precision_score(y_deploy, y_pred_dt_BGKN, average=\"macro\"),\n",
    "               metrics.precision_score(y_deploy, y_pred_dt_BGDT, average=\"macro\"),\n",
    "               metrics.precision_score(y_deploy, y_pred_dt_RF, average=\"macro\"),\n",
    "               metrics.precision_score(y_deploy, y_pred_dt_AD, average=\"macro\"),\n",
    "               metrics.precision_score(y_deploy, y_pred_NB, average=\"macro\"),\n",
    "               metrics.precision_score(y_deploy, y_pred_dt_LDA, average=\"macro\"),\n",
    "               metrics.precision_score(y_deploy, y_pred_dt_QDA, average=\"macro\"),              \n",
    "               metrics.precision_score(y_deploy, y_pred_LR, average=\"macro\"),\n",
    "               metrics.precision_score(y_deploy, y_pred_GP, average=\"macro\"),\n",
    "               metrics.precision_score(y_deploy, y_pred_gbm, average=\"macro\"),\n",
    "               metrics.precision_score(y_deploy, y_pred_xgboost, average=\"macro\"),\n",
    "               metrics.precision_score(y_deploy, y_pred_NN, average=\"macro\"),\n",
    "               metrics.precision_score(y_deploy, y_pred_EG, average=\"macro\"),\n",
    "               metrics.precision_score(y_deploy, y_pred_AEG, average=\"macro\")]\n",
    "\n",
    "RecallData = [ metrics.recall_score(y_deploy, y_pred_neigh, average=\"macro\"),\n",
    "              metrics.recall_score(y_deploy, y_pred_svm_linear, average=\"macro\"),\n",
    "              metrics.recall_score(y_deploy, y_pred_svm, average=\"macro\"),\n",
    "              metrics.recall_score(y_deploy, y_pred_gtgini, average=\"macro\"),\n",
    "              metrics.recall_score(y_deploy, y_pred_dt_IG, average=\"macro\"),\n",
    "              metrics.recall_score(y_deploy, y_pred_dt_BGKN, average=\"macro\"),\n",
    "              metrics.recall_score(y_deploy, y_pred_dt_BGDT, average=\"macro\"),\n",
    "              metrics.recall_score(y_deploy, y_pred_dt_RF, average=\"macro\"),\n",
    "              metrics.recall_score(y_deploy, y_pred_dt_AD, average=\"macro\"),\n",
    "              metrics.recall_score(y_deploy, y_pred_NB, average=\"macro\"),\n",
    "              metrics.recall_score(y_deploy, y_pred_dt_LDA, average=\"macro\"),\n",
    "              metrics.recall_score(y_deploy, y_pred_dt_QDA, average=\"macro\"),\n",
    "              metrics.recall_score(y_deploy, y_pred_LR, average=\"macro\"),\n",
    "              metrics.recall_score(y_deploy, y_pred_GP, average=\"macro\"),\n",
    "              metrics.recall_score(y_deploy, y_pred_gbm, average=\"macro\"),\n",
    "              metrics.recall_score(y_deploy, y_pred_xgboost, average=\"macro\"),       \n",
    "              metrics.recall_score(y_deploy, y_pred_NN, average=\"macro\"),\n",
    "              metrics.recall_score(y_deploy, y_pred_EG, average=\"macro\"),\n",
    "              metrics.recall_score(y_deploy, y_pred_AEG, average=\"macro\")]\n",
    "\n",
    "F1Data = [metrics.f1_score(y_deploy, y_pred_neigh, average=\"macro\"),\n",
    "          metrics.f1_score(y_deploy, y_pred_svm_linear, average=\"macro\"),\n",
    "          metrics.f1_score(y_deploy, y_pred_svm, average=\"macro\"),\n",
    "          metrics.f1_score(y_deploy, y_pred_gtgini, average=\"macro\"),\n",
    "          metrics.f1_score(y_deploy, y_pred_dt_IG, average=\"macro\"),\n",
    "          metrics.f1_score(y_deploy, y_pred_dt_BGKN, average=\"macro\"),\n",
    "          metrics.f1_score(y_deploy, y_pred_dt_BGDT, average=\"macro\"),\n",
    "          metrics.f1_score(y_deploy, y_pred_dt_RF, average=\"macro\"),\n",
    "          metrics.f1_score(y_deploy, y_pred_dt_AD, average=\"macro\"),\n",
    "          metrics.f1_score(y_deploy, y_pred_NB, average=\"macro\"),\n",
    "          metrics.f1_score(y_deploy, y_pred_dt_LDA, average=\"macro\"),\n",
    "          metrics.f1_score(y_deploy, y_pred_dt_QDA, average=\"macro\"),         \n",
    "          metrics.f1_score(y_deploy, y_pred_LR, average=\"macro\"),\n",
    "          metrics.f1_score(y_deploy, y_pred_GP, average=\"macro\"),\n",
    "          metrics.f1_score(y_deploy, y_pred_gbm, average=\"macro\"),\n",
    "          metrics.f1_score(y_deploy, y_pred_xgboost, average=\"macro\"),\n",
    "          metrics.f1_score(y_deploy, y_pred_NN, average=\"macro\"),                             \n",
    "          metrics.f1_score(y_deploy, y_pred_EG, average=\"macro\"),\n",
    "          metrics.f1_score(y_deploy, y_pred_AEG, average=\"macro\")]         \n",
    "\n",
    "\n",
    "\n",
    "N = len(accData)\n",
    "## necessary variables\n",
    "ind = np.arange(N)                # the x locations for the groups\n",
    "width = 0.17                     # the width of the bars\n",
    "## the bars\n",
    "rects1 = ax.bar(ind, accData, width,\n",
    "                color='black',\n",
    "                #yerr=menStd,\n",
    "                error_kw=dict(elinewidth=2,ecolor='red'))\n",
    "rects2 = ax.bar(ind+width, F1Data, width,\n",
    "                    color='red',\n",
    "                    #yerr=womenStd,\n",
    "                    error_kw=dict(elinewidth=2,ecolor='black'))\n",
    "rects3 = ax.bar(ind+width+width, PresionData, width,\n",
    "                    color='green',\n",
    "                    #yerr=womenStd,\n",
    "                    error_kw=dict(elinewidth=2,ecolor='blue'))\n",
    "rects4 = ax.bar(ind+width+width+width, RecallData, width,\n",
    "                    color='blue',\n",
    "                    #yerr=womenStd,\n",
    "                    error_kw=dict(elinewidth=2,ecolor='green'))\n",
    "# axes and labels\n",
    "ax.set_xlim(-width,len(ind)+width)\n",
    "ax.set_ylim(0,1)\n",
    "ax.set_ylabel('Score')\n",
    "ax.set_xlabel('Name of Classifier')\n",
    "ax.set_title('Scores of different classifiers on Test Data')\n",
    "xTickMarks = ['Knn', 'LSVM', 'SVM', 'DT_gini', 'DT_entorpy' ,\n",
    "              'Bagging Knn' , 'Bagging DT' , 'Random Forest' , 'Ada Boost' ,\n",
    "              'NB' , 'LDA' , 'QDA' ,'Log. Reg.' ,'GP Class.','LightGBM','Xgboost',\n",
    "              'NN',  'E Greedy' , 'Decay E Gr.' ]\n",
    "ax.set_xticks(ind+width)\n",
    "xtickNames = ax.set_xticklabels(xTickMarks)\n",
    "plt.setp(xtickNames, rotation=90, fontsize=10)\n",
    "## add a legend\n",
    "ax.legend( (rects1[0], rects2[0], rects3[0], rects4[0]), ('Acc.', 'F1' , 'Prec.' , 'Recall') , loc=8, fancybox=True, \n",
    "          frameon=True, shadow=True)\n",
    "ax.set_facecolor('0.9')\n",
    "\n",
    "ax.spines['top'].set_visible(True)\n",
    "ax.spines['right'].set_visible(True)\n",
    "ax.spines['bottom'].set_visible(True)\n",
    "ax.spines['left'].set_visible(True)\n",
    "\n",
    "ax.spines['top'].set_linewidth(0.9)\n",
    "ax.spines['right'].set_linewidth(0.9)\n",
    "ax.spines['bottom'].set_linewidth(0.9)\n",
    "ax.spines['left'].set_linewidth(0.9)\n",
    "ax.grid(False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from math import pi\n",
    "from bokeh.charts import Bar, Area, defaults\n",
    "from bokeh.layouts import row\n",
    "from bokeh.charts.attributes import cat, color\n",
    "from bokeh.charts.operations import blend\n",
    "#from bokeh.charts.utils import df_from_json\n",
    "from bokeh.plotting import figure, output_notebook, show\n",
    "############################################################################################################\n",
    "TOOLS = 'box_zoom,box_select,crosshair,resize,reset,lasso_select,pan,save,poly_select,tap,wheel_zoom,undo'\n",
    "#defaults.width = 1000\n",
    "#defaults.height = 800\n",
    "output_notebook()\n",
    "df1 = pd.DataFrame({'Matric': xTickMarks,\n",
    "                    'Accuracy':accData, \n",
    "                   'Precision': PresionData, \n",
    "                   'Recall': RecallData, \n",
    "                    'F1 Score': F1Data})\n",
    "############################################################################################################\n",
    "bar = Bar(df1,\n",
    "          values=blend('Accuracy', 'F1 Score', 'Precision','Recall', name='Scores', labels_name='Score'),\n",
    "          label=cat(columns='Matric', sort=False),\n",
    "          stack=cat(columns='Score', sort=False),\n",
    "          color=color(columns='Score', palette=['SaddleBrown', 'Silver', 'Goldenrod', 'Grey'],\n",
    "                      sort=False),\n",
    "          legend='bottom_center', xlabel=\"List of Models\", ylabel=\"The Scores\",\n",
    "          title=\"Scores of different Models\", \n",
    "          tooltips=[('Score', '@Score'), ('Model', '@Matric')],\n",
    "          tools=TOOLS, plot_width=900, plot_height=800)\n",
    "bar.title.align = \"center\"\n",
    "bar.xaxis.major_label_orientation = pi/2\n",
    "###############################################################################################################\n",
    "p = Bar(df1, label='Matric', \n",
    "        values = blend('Accuracy', 'F1 Score', 'Precision','Recall', name='Scores', labels_name='Score'),\n",
    "        group=cat(columns='Score', sort=False),\n",
    "        title=\"Scores of different Models\", legend='bottom_center',\n",
    "       tools=TOOLS, plot_width=900, plot_height=600,\n",
    "       xlabel='List of Models', ylabel='The Scores')\n",
    "p.title.align = \"center\"\n",
    "#p.yaxis.major_label_orientation = \"vertical\"\n",
    "p.xaxis.major_label_orientation = pi/2\n",
    "#########################################################################################################\n",
    "data = dict(\n",
    "    Acc = accData,\n",
    "    Pre = PresionData,\n",
    "    Rec = RecallData,\n",
    "    F1 = F1Data,\n",
    ")\n",
    "area1 = Area(data, title=\"The trend of score over Models\", legend=\"bottom_center\",\n",
    "             xlabel='List of Models', ylabel='The Scores',\n",
    "            tools=TOOLS, plot_width=450, plot_height=300)\n",
    "area1.title.align = \"center\"\n",
    "area2 = Area(data, title=\"The trend of score over Models\", legend=\"bottom_center\",\n",
    "             stack=True, xlabel='List of Models', ylabel='The Scores',\n",
    "            tools=TOOLS, plot_width=450, plot_height=300)\n",
    "area2.title.align = \"center\"\n",
    "#########################################################################################################\n",
    "show(bar)\n",
    "show(p)\n",
    "#show(area1)\n",
    "#show(area2)\n",
    "show(row(area1, area2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compare Softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "accData = [metrics.accuracy_score(y_deploy, y_pred_neigh),\n",
    "           metrics.accuracy_score(y_deploy, y_pred_svm_linear), \n",
    "           metrics.accuracy_score(y_deploy, y_pred_svm), \n",
    "           metrics.accuracy_score(y_deploy, y_pred_gtgini),\n",
    "           metrics.accuracy_score(y_deploy, y_pred_dt_IG), \n",
    "           metrics.accuracy_score(y_deploy, y_pred_dt_BGKN),\n",
    "           metrics.accuracy_score(y_deploy, y_pred_dt_BGDT), \n",
    "           metrics.accuracy_score(y_deploy, y_pred_dt_RF),\n",
    "           metrics.accuracy_score(y_deploy, y_pred_dt_AD), \n",
    "           metrics.accuracy_score(y_deploy, y_pred_NB),\n",
    "           metrics.accuracy_score(y_deploy, y_pred_dt_LDA), \n",
    "           metrics.accuracy_score(y_deploy, y_pred_dt_QDA),           \n",
    "           metrics.accuracy_score(y_deploy, y_pred_LR),\n",
    "           metrics.accuracy_score(y_deploy, y_pred_GP),\n",
    "           metrics.accuracy_score(y_deploy, y_pred_gbm),\n",
    "           metrics.accuracy_score(y_deploy, y_pred_xgboost),\n",
    "           metrics.accuracy_score(y_deploy, y_pred_NN),        \n",
    "           metrics.accuracy_score(y_deploy, y_pred_SM),\n",
    "           metrics.accuracy_score(y_deploy, y_pred_ASM)]\n",
    "          \n",
    "PresionData = [metrics.precision_score(y_deploy, y_pred_neigh, average=\"macro\"),\n",
    "               metrics.precision_score(y_deploy, y_pred_svm_linear, average=\"macro\"),\n",
    "               metrics.precision_score(y_deploy, y_pred_svm, average=\"macro\"),\n",
    "               metrics.precision_score(y_deploy, y_pred_gtgini, average=\"macro\"),\n",
    "               metrics.precision_score(y_deploy, y_pred_dt_IG, average=\"macro\"),\n",
    "               metrics.precision_score(y_deploy, y_pred_dt_BGKN, average=\"macro\"),\n",
    "               metrics.precision_score(y_deploy, y_pred_dt_BGDT, average=\"macro\"),\n",
    "               metrics.precision_score(y_deploy, y_pred_dt_RF, average=\"macro\"),\n",
    "               metrics.precision_score(y_deploy, y_pred_dt_AD, average=\"macro\"),\n",
    "               metrics.precision_score(y_deploy, y_pred_NB, average=\"macro\"),\n",
    "               metrics.precision_score(y_deploy, y_pred_dt_LDA, average=\"macro\"),\n",
    "               metrics.precision_score(y_deploy, y_pred_dt_QDA, average=\"macro\"),              \n",
    "               metrics.precision_score(y_deploy, y_pred_LR, average=\"macro\"),\n",
    "               metrics.precision_score(y_deploy, y_pred_GP, average=\"macro\"),\n",
    "               metrics.precision_score(y_deploy, y_pred_gbm, average=\"macro\"),\n",
    "               metrics.precision_score(y_deploy, y_pred_xgboost, average=\"macro\"),\n",
    "               metrics.precision_score(y_deploy, y_pred_NN, average=\"macro\"),\n",
    "               metrics.precision_score(y_deploy, y_pred_SM, average=\"macro\"),\n",
    "               metrics.precision_score(y_deploy, y_pred_ASM, average=\"macro\")]\n",
    "\n",
    "RecallData = [ metrics.recall_score(y_deploy, y_pred_neigh, average=\"macro\"),\n",
    "              metrics.recall_score(y_deploy, y_pred_svm_linear, average=\"macro\"),\n",
    "              metrics.recall_score(y_deploy, y_pred_svm, average=\"macro\"),\n",
    "              metrics.recall_score(y_deploy, y_pred_gtgini, average=\"macro\"),\n",
    "              metrics.recall_score(y_deploy, y_pred_dt_IG, average=\"macro\"),\n",
    "              metrics.recall_score(y_deploy, y_pred_dt_BGKN, average=\"macro\"),\n",
    "              metrics.recall_score(y_deploy, y_pred_dt_BGDT, average=\"macro\"),\n",
    "              metrics.recall_score(y_deploy, y_pred_dt_RF, average=\"macro\"),\n",
    "              metrics.recall_score(y_deploy, y_pred_dt_AD, average=\"macro\"),\n",
    "              metrics.recall_score(y_deploy, y_pred_NB, average=\"macro\"),\n",
    "              metrics.recall_score(y_deploy, y_pred_dt_LDA, average=\"macro\"),\n",
    "              metrics.recall_score(y_deploy, y_pred_dt_QDA, average=\"macro\"),\n",
    "              metrics.recall_score(y_deploy, y_pred_LR, average=\"macro\"),\n",
    "              metrics.recall_score(y_deploy, y_pred_GP, average=\"macro\"),\n",
    "              metrics.recall_score(y_deploy, y_pred_gbm, average=\"macro\"),\n",
    "              metrics.recall_score(y_deploy, y_pred_xgboost, average=\"macro\"),       \n",
    "              metrics.recall_score(y_deploy, y_pred_NN, average=\"macro\"),\n",
    "              metrics.recall_score(y_deploy, y_pred_SM, average=\"macro\"),\n",
    "              metrics.recall_score(y_deploy, y_pred_ASM, average=\"macro\")]\n",
    "\n",
    "F1Data = [metrics.f1_score(y_deploy, y_pred_neigh, average=\"macro\"),\n",
    "          metrics.f1_score(y_deploy, y_pred_svm_linear, average=\"macro\"),\n",
    "          metrics.f1_score(y_deploy, y_pred_svm, average=\"macro\"),\n",
    "          metrics.f1_score(y_deploy, y_pred_gtgini, average=\"macro\"),\n",
    "          metrics.f1_score(y_deploy, y_pred_dt_IG, average=\"macro\"),\n",
    "          metrics.f1_score(y_deploy, y_pred_dt_BGKN, average=\"macro\"),\n",
    "          metrics.f1_score(y_deploy, y_pred_dt_BGDT, average=\"macro\"),\n",
    "          metrics.f1_score(y_deploy, y_pred_dt_RF, average=\"macro\"),\n",
    "          metrics.f1_score(y_deploy, y_pred_dt_AD, average=\"macro\"),\n",
    "          metrics.f1_score(y_deploy, y_pred_NB, average=\"macro\"),\n",
    "          metrics.f1_score(y_deploy, y_pred_dt_LDA, average=\"macro\"),\n",
    "          metrics.f1_score(y_deploy, y_pred_dt_QDA, average=\"macro\"),         \n",
    "          metrics.f1_score(y_deploy, y_pred_LR, average=\"macro\"),\n",
    "          metrics.f1_score(y_deploy, y_pred_GP, average=\"macro\"),\n",
    "          metrics.f1_score(y_deploy, y_pred_gbm, average=\"macro\"),\n",
    "          metrics.f1_score(y_deploy, y_pred_xgboost, average=\"macro\"),\n",
    "          metrics.f1_score(y_deploy, y_pred_NN, average=\"macro\"),  \n",
    "          metrics.f1_score(y_deploy, y_pred_SM, average=\"macro\"),\n",
    "          metrics.f1_score(y_deploy, y_pred_ASM, average=\"macro\")]              \n",
    "\n",
    "\n",
    "\n",
    "N = len(accData)\n",
    "## necessary variables\n",
    "ind = np.arange(N)                # the x locations for the groups\n",
    "width = 0.17                     # the width of the bars\n",
    "## the bars\n",
    "rects1 = ax.bar(ind, accData, width,\n",
    "                color='black',\n",
    "                #yerr=menStd,\n",
    "                error_kw=dict(elinewidth=2,ecolor='red'))\n",
    "rects2 = ax.bar(ind+width, F1Data, width,\n",
    "                    color='red',\n",
    "                    #yerr=womenStd,\n",
    "                    error_kw=dict(elinewidth=2,ecolor='black'))\n",
    "rects3 = ax.bar(ind+width+width, PresionData, width,\n",
    "                    color='green',\n",
    "                    #yerr=womenStd,\n",
    "                    error_kw=dict(elinewidth=2,ecolor='blue'))\n",
    "rects4 = ax.bar(ind+width+width+width, RecallData, width,\n",
    "                    color='blue',\n",
    "                    #yerr=womenStd,\n",
    "                    error_kw=dict(elinewidth=2,ecolor='green'))\n",
    "# axes and labels\n",
    "ax.set_xlim(-width,len(ind)+width)\n",
    "ax.set_ylim(0,1)\n",
    "ax.set_ylabel('Score')\n",
    "ax.set_xlabel('Name of Classifier')\n",
    "ax.set_title('Scores of different classifiers on Test Data')\n",
    "xTickMarks = ['Knn', 'LSVM', 'SVM', 'DT_gini', 'DT_entorpy' ,\n",
    "              'Bagging Knn' , 'Bagging DT' , 'Random Forest' , 'Ada Boost' ,\n",
    "              'NB' , 'LDA' , 'QDA' ,'Log. Reg.' ,'GP Class.','LightGBM','Xgboost',\n",
    "              'NN',  'Softmax', 'Decay SM']\n",
    "ax.set_xticks(ind+width)\n",
    "xtickNames = ax.set_xticklabels(xTickMarks)\n",
    "plt.setp(xtickNames, rotation=90, fontsize=10)\n",
    "## add a legend\n",
    "ax.legend( (rects1[0], rects2[0], rects3[0], rects4[0]), ('Acc.', 'F1' , 'Prec.' , 'Recall') , loc=8, fancybox=True, \n",
    "          frameon=True, shadow=True)\n",
    "ax.set_facecolor('0.9')\n",
    "\n",
    "ax.spines['top'].set_visible(True)\n",
    "ax.spines['right'].set_visible(True)\n",
    "ax.spines['bottom'].set_visible(True)\n",
    "ax.spines['left'].set_visible(True)\n",
    "\n",
    "ax.spines['top'].set_linewidth(0.9)\n",
    "ax.spines['right'].set_linewidth(0.9)\n",
    "ax.spines['bottom'].set_linewidth(0.9)\n",
    "ax.spines['left'].set_linewidth(0.9)\n",
    "ax.grid(False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from math import pi\n",
    "from bokeh.charts import Bar, Area, defaults\n",
    "from bokeh.layouts import row\n",
    "from bokeh.charts.attributes import cat, color\n",
    "from bokeh.charts.operations import blend\n",
    "#from bokeh.charts.utils import df_from_json\n",
    "from bokeh.plotting import figure, output_notebook, show\n",
    "############################################################################################################\n",
    "TOOLS = 'box_zoom,box_select,crosshair,resize,reset,lasso_select,pan,save,poly_select,tap,wheel_zoom,undo'\n",
    "#defaults.width = 1000\n",
    "#defaults.height = 800\n",
    "output_notebook()\n",
    "df1 = pd.DataFrame({'Matric': xTickMarks,\n",
    "                    'Accuracy':accData, \n",
    "                   'Precision': PresionData, \n",
    "                   'Recall': RecallData, \n",
    "                    'F1 Score': F1Data})\n",
    "############################################################################################################\n",
    "bar = Bar(df1,\n",
    "          values=blend('Accuracy', 'F1 Score', 'Precision','Recall', name='Scores', labels_name='Score'),\n",
    "          label=cat(columns='Matric', sort=False),\n",
    "          stack=cat(columns='Score', sort=False),\n",
    "          color=color(columns='Score', palette=['SaddleBrown', 'Silver', 'Goldenrod', 'Grey'],\n",
    "                      sort=False),\n",
    "          legend='bottom_center', xlabel=\"List of Models\", ylabel=\"The Scores\",\n",
    "          title=\"Scores of different Models\", \n",
    "          tooltips=[('Score', '@Score'), ('Model', '@Matric')],\n",
    "          tools=TOOLS, plot_width=900, plot_height=800)\n",
    "bar.title.align = \"center\"\n",
    "bar.xaxis.major_label_orientation = pi/2\n",
    "###############################################################################################################\n",
    "p = Bar(df1, label='Matric', \n",
    "        values = blend('Accuracy', 'F1 Score', 'Precision','Recall', name='Scores', labels_name='Score'),\n",
    "        group=cat(columns='Score', sort=False),\n",
    "        title=\"Scores of different Models\", legend='bottom_center',\n",
    "       tools=TOOLS, plot_width=900, plot_height=600,\n",
    "       xlabel='List of Models', ylabel='The Scores')\n",
    "p.title.align = \"center\"\n",
    "#p.yaxis.major_label_orientation = \"vertical\"\n",
    "p.xaxis.major_label_orientation = pi/2\n",
    "#########################################################################################################\n",
    "data = dict(\n",
    "    Acc = accData,\n",
    "    Pre = PresionData,\n",
    "    Rec = RecallData,\n",
    "    F1 = F1Data,\n",
    ")\n",
    "area1 = Area(data, title=\"The trend of score over Models\", legend=\"bottom_center\",\n",
    "             xlabel='List of Models', ylabel='The Scores',\n",
    "            tools=TOOLS, plot_width=450, plot_height=300)\n",
    "area1.title.align = \"center\"\n",
    "area2 = Area(data, title=\"The trend of score over Models\", legend=\"bottom_center\",\n",
    "             stack=True, xlabel='List of Models', ylabel='The Scores',\n",
    "            tools=TOOLS, plot_width=450, plot_height=300)\n",
    "area2.title.align = \"center\"\n",
    "#########################################################################################################\n",
    "show(bar)\n",
    "show(p)\n",
    "#show(area1)\n",
    "#show(area2)\n",
    "show(row(area1, area2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compare Esiplon greedy and Softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "accData = [metrics.accuracy_score(y_deploy, y_pred_neigh),\n",
    "           metrics.accuracy_score(y_deploy, y_pred_svm_linear), \n",
    "           metrics.accuracy_score(y_deploy, y_pred_svm), \n",
    "           metrics.accuracy_score(y_deploy, y_pred_gtgini),\n",
    "           metrics.accuracy_score(y_deploy, y_pred_dt_IG), \n",
    "           metrics.accuracy_score(y_deploy, y_pred_dt_BGKN),\n",
    "           metrics.accuracy_score(y_deploy, y_pred_dt_BGDT), \n",
    "           metrics.accuracy_score(y_deploy, y_pred_dt_RF),\n",
    "           metrics.accuracy_score(y_deploy, y_pred_dt_AD), \n",
    "           metrics.accuracy_score(y_deploy, y_pred_NB),\n",
    "           metrics.accuracy_score(y_deploy, y_pred_dt_LDA), \n",
    "           metrics.accuracy_score(y_deploy, y_pred_dt_QDA),           \n",
    "           metrics.accuracy_score(y_deploy, y_pred_LR),\n",
    "           metrics.accuracy_score(y_deploy, y_pred_GP),\n",
    "           metrics.accuracy_score(y_deploy, y_pred_gbm),\n",
    "           metrics.accuracy_score(y_deploy, y_pred_xgboost),\n",
    "           metrics.accuracy_score(y_deploy, y_pred_NN),        \n",
    "           metrics.accuracy_score(y_deploy, y_pred_EG),\n",
    "           metrics.accuracy_score(y_deploy, y_pred_AEG),\n",
    "           metrics.accuracy_score(y_deploy, y_pred_SM),\n",
    "           metrics.accuracy_score(y_deploy, y_pred_ASM)]\n",
    "          \n",
    "PresionData = [metrics.precision_score(y_deploy, y_pred_neigh, average=\"macro\"),\n",
    "               metrics.precision_score(y_deploy, y_pred_svm_linear, average=\"macro\"),\n",
    "               metrics.precision_score(y_deploy, y_pred_svm, average=\"macro\"),\n",
    "               metrics.precision_score(y_deploy, y_pred_gtgini, average=\"macro\"),\n",
    "               metrics.precision_score(y_deploy, y_pred_dt_IG, average=\"macro\"),\n",
    "               metrics.precision_score(y_deploy, y_pred_dt_BGKN, average=\"macro\"),\n",
    "               metrics.precision_score(y_deploy, y_pred_dt_BGDT, average=\"macro\"),\n",
    "               metrics.precision_score(y_deploy, y_pred_dt_RF, average=\"macro\"),\n",
    "               metrics.precision_score(y_deploy, y_pred_dt_AD, average=\"macro\"),\n",
    "               metrics.precision_score(y_deploy, y_pred_NB, average=\"macro\"),\n",
    "               metrics.precision_score(y_deploy, y_pred_dt_LDA, average=\"macro\"),\n",
    "               metrics.precision_score(y_deploy, y_pred_dt_QDA, average=\"macro\"),              \n",
    "               metrics.precision_score(y_deploy, y_pred_LR, average=\"macro\"),\n",
    "               metrics.precision_score(y_deploy, y_pred_GP, average=\"macro\"),\n",
    "               metrics.precision_score(y_deploy, y_pred_gbm, average=\"macro\"),\n",
    "               metrics.precision_score(y_deploy, y_pred_xgboost, average=\"macro\"),\n",
    "               metrics.precision_score(y_deploy, y_pred_NN, average=\"macro\"),\n",
    "               metrics.precision_score(y_deploy, y_pred_EG, average=\"macro\"),\n",
    "               metrics.precision_score(y_deploy, y_pred_AEG, average=\"macro\"),\n",
    "               metrics.precision_score(y_deploy, y_pred_SM, average=\"macro\"),\n",
    "               metrics.precision_score(y_deploy, y_pred_ASM, average=\"macro\")]\n",
    "\n",
    "RecallData = [ metrics.recall_score(y_deploy, y_pred_neigh, average=\"macro\"),\n",
    "              metrics.recall_score(y_deploy, y_pred_svm_linear, average=\"macro\"),\n",
    "              metrics.recall_score(y_deploy, y_pred_svm, average=\"macro\"),\n",
    "              metrics.recall_score(y_deploy, y_pred_gtgini, average=\"macro\"),\n",
    "              metrics.recall_score(y_deploy, y_pred_dt_IG, average=\"macro\"),\n",
    "              metrics.recall_score(y_deploy, y_pred_dt_BGKN, average=\"macro\"),\n",
    "              metrics.recall_score(y_deploy, y_pred_dt_BGDT, average=\"macro\"),\n",
    "              metrics.recall_score(y_deploy, y_pred_dt_RF, average=\"macro\"),\n",
    "              metrics.recall_score(y_deploy, y_pred_dt_AD, average=\"macro\"),\n",
    "              metrics.recall_score(y_deploy, y_pred_NB, average=\"macro\"),\n",
    "              metrics.recall_score(y_deploy, y_pred_dt_LDA, average=\"macro\"),\n",
    "              metrics.recall_score(y_deploy, y_pred_dt_QDA, average=\"macro\"),\n",
    "              metrics.recall_score(y_deploy, y_pred_LR, average=\"macro\"),\n",
    "              metrics.recall_score(y_deploy, y_pred_GP, average=\"macro\"),\n",
    "              metrics.recall_score(y_deploy, y_pred_gbm, average=\"macro\"),\n",
    "              metrics.recall_score(y_deploy, y_pred_xgboost, average=\"macro\"),       \n",
    "              metrics.recall_score(y_deploy, y_pred_NN, average=\"macro\"),\n",
    "              metrics.recall_score(y_deploy, y_pred_EG, average=\"macro\"),\n",
    "              metrics.recall_score(y_deploy, y_pred_AEG, average=\"macro\"),\n",
    "              metrics.recall_score(y_deploy, y_pred_SM, average=\"macro\"),\n",
    "              metrics.recall_score(y_deploy, y_pred_ASM, average=\"macro\")]\n",
    "\n",
    "F1Data = [metrics.f1_score(y_deploy, y_pred_neigh, average=\"macro\"),\n",
    "          metrics.f1_score(y_deploy, y_pred_svm_linear, average=\"macro\"),\n",
    "          metrics.f1_score(y_deploy, y_pred_svm, average=\"macro\"),\n",
    "          metrics.f1_score(y_deploy, y_pred_gtgini, average=\"macro\"),\n",
    "          metrics.f1_score(y_deploy, y_pred_dt_IG, average=\"macro\"),\n",
    "          metrics.f1_score(y_deploy, y_pred_dt_BGKN, average=\"macro\"),\n",
    "          metrics.f1_score(y_deploy, y_pred_dt_BGDT, average=\"macro\"),\n",
    "          metrics.f1_score(y_deploy, y_pred_dt_RF, average=\"macro\"),\n",
    "          metrics.f1_score(y_deploy, y_pred_dt_AD, average=\"macro\"),\n",
    "          metrics.f1_score(y_deploy, y_pred_NB, average=\"macro\"),\n",
    "          metrics.f1_score(y_deploy, y_pred_dt_LDA, average=\"macro\"),\n",
    "          metrics.f1_score(y_deploy, y_pred_dt_QDA, average=\"macro\"),         \n",
    "          metrics.f1_score(y_deploy, y_pred_LR, average=\"macro\"),\n",
    "          metrics.f1_score(y_deploy, y_pred_GP, average=\"macro\"),\n",
    "          metrics.f1_score(y_deploy, y_pred_gbm, average=\"macro\"),\n",
    "          metrics.f1_score(y_deploy, y_pred_xgboost, average=\"macro\"),\n",
    "          metrics.f1_score(y_deploy, y_pred_NN, average=\"macro\"),                             \n",
    "          metrics.f1_score(y_deploy, y_pred_EG, average=\"macro\"),\n",
    "          metrics.f1_score(y_deploy, y_pred_AEG, average=\"macro\"),\n",
    "          metrics.f1_score(y_deploy, y_pred_SM, average=\"macro\"),\n",
    "          metrics.f1_score(y_deploy, y_pred_ASM, average=\"macro\")]         \n",
    "\n",
    "\n",
    "\n",
    "N = len(accData)\n",
    "## necessary variables\n",
    "ind = np.arange(N)                # the x locations for the groups\n",
    "width = 0.17                     # the width of the bars\n",
    "## the bars\n",
    "rects1 = ax.bar(ind, accData, width,\n",
    "                color='black',\n",
    "                #yerr=menStd,\n",
    "                error_kw=dict(elinewidth=2,ecolor='red'))\n",
    "rects2 = ax.bar(ind+width, F1Data, width,\n",
    "                    color='red',\n",
    "                    #yerr=womenStd,\n",
    "                    error_kw=dict(elinewidth=2,ecolor='black'))\n",
    "rects3 = ax.bar(ind+width+width, PresionData, width,\n",
    "                    color='green',\n",
    "                    #yerr=womenStd,\n",
    "                    error_kw=dict(elinewidth=2,ecolor='blue'))\n",
    "rects4 = ax.bar(ind+width+width+width, RecallData, width,\n",
    "                    color='blue',\n",
    "                    #yerr=womenStd,\n",
    "                    error_kw=dict(elinewidth=2,ecolor='green'))\n",
    "# axes and labels\n",
    "ax.set_xlim(-width,len(ind)+width)\n",
    "ax.set_ylim(0,1)\n",
    "ax.set_ylabel('Score')\n",
    "ax.set_xlabel('Name of Classifier')\n",
    "ax.set_title('Scores of different classifiers on Test Data')\n",
    "xTickMarks = ['Knn', 'LSVM', 'SVM', 'DT_gini', 'DT_entorpy' ,\n",
    "              'Bagging Knn' , 'Bagging DT' , 'Random Forest' , 'Ada Boost' ,\n",
    "              'NB' , 'LDA' , 'QDA' ,'Log. Reg.' ,'GP Class.','LightGBM','Xgboost',\n",
    "              'NN',  'E Greedy' , 'Decay E Gr.' , 'Softmax', 'Decay SM']\n",
    "ax.set_xticks(ind+width)\n",
    "xtickNames = ax.set_xticklabels(xTickMarks)\n",
    "plt.setp(xtickNames, rotation=90, fontsize=10)\n",
    "## add a legend\n",
    "ax.legend( (rects1[0], rects2[0], rects3[0], rects4[0]), ('Acc.', 'F1' , 'Prec.' , 'Recall') , loc=8, fancybox=True, \n",
    "          frameon=True, shadow=True)\n",
    "ax.set_facecolor('0.9')\n",
    "\n",
    "ax.spines['top'].set_visible(True)\n",
    "ax.spines['right'].set_visible(True)\n",
    "ax.spines['bottom'].set_visible(True)\n",
    "ax.spines['left'].set_visible(True)\n",
    "\n",
    "ax.spines['top'].set_linewidth(0.9)\n",
    "ax.spines['right'].set_linewidth(0.9)\n",
    "ax.spines['bottom'].set_linewidth(0.9)\n",
    "ax.spines['left'].set_linewidth(0.9)\n",
    "ax.grid(False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from math import pi\n",
    "from bokeh.charts import Bar, Area, defaults\n",
    "from bokeh.layouts import row\n",
    "from bokeh.charts.attributes import cat, color\n",
    "from bokeh.charts.operations import blend\n",
    "#from bokeh.charts.utils import df_from_json\n",
    "from bokeh.plotting import figure, output_notebook, show\n",
    "############################################################################################################\n",
    "TOOLS = 'box_zoom,box_select,crosshair,resize,reset,lasso_select,pan,save,poly_select,tap,wheel_zoom,undo'\n",
    "#defaults.width = 1000\n",
    "#defaults.height = 800\n",
    "output_notebook()\n",
    "df1 = pd.DataFrame({'Matric': xTickMarks,\n",
    "                    'Accuracy':accData, \n",
    "                   'Precision': PresionData, \n",
    "                   'Recall': RecallData, \n",
    "                    'F1 Score': F1Data})\n",
    "############################################################################################################\n",
    "bar = Bar(df1,\n",
    "          values=blend('Accuracy', 'F1 Score', 'Precision','Recall', name='Scores', labels_name='Score'),\n",
    "          label=cat(columns='Matric', sort=False),\n",
    "          stack=cat(columns='Score', sort=False),\n",
    "          color=color(columns='Score', palette=['SaddleBrown', 'Silver', 'Goldenrod', 'Grey'],\n",
    "                      sort=False),\n",
    "          legend='bottom_center', xlabel=\"List of Models\", ylabel=\"The Scores\",\n",
    "          title=\"Scores of different Models\", \n",
    "          tooltips=[('Score', '@Score'), ('Model', '@Matric')],\n",
    "          tools=TOOLS, plot_width=900, plot_height=800)\n",
    "bar.title.align = \"center\"\n",
    "bar.xaxis.major_label_orientation = pi/2\n",
    "###############################################################################################################\n",
    "p = Bar(df1, label='Matric', \n",
    "        values = blend('Accuracy', 'F1 Score', 'Precision','Recall', name='Scores', labels_name='Score'),\n",
    "        group=cat(columns='Score', sort=False),\n",
    "        title=\"Scores of different Models\", legend='bottom_center',\n",
    "       tools=TOOLS, plot_width=900, plot_height=600,\n",
    "       xlabel='List of Models', ylabel='The Scores')\n",
    "p.title.align = \"center\"\n",
    "#p.yaxis.major_label_orientation = \"vertical\"\n",
    "p.xaxis.major_label_orientation = pi/2\n",
    "#########################################################################################################\n",
    "data = dict(\n",
    "    Acc = accData,\n",
    "    Pre = PresionData,\n",
    "    Rec = RecallData,\n",
    "    F1 = F1Data,\n",
    ")\n",
    "area1 = Area(data, title=\"The trend of score over Models\", legend=\"bottom_center\",\n",
    "             xlabel='List of Models', ylabel='The Scores',\n",
    "            tools=TOOLS, plot_width=450, plot_height=300)\n",
    "area1.title.align = \"center\"\n",
    "area2 = Area(data, title=\"The trend of score over Models\", legend=\"bottom_center\",\n",
    "             stack=True, xlabel='List of Models', ylabel='The Scores',\n",
    "            tools=TOOLS, plot_width=450, plot_height=300)\n",
    "area2.title.align = \"center\"\n",
    "#########################################################################################################\n",
    "show(bar)\n",
    "show(p)\n",
    "#show(area1)\n",
    "#show(area2)\n",
    "show(row(area1, area2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hedge and EXP3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "accData = [metrics.accuracy_score(y_deploy, y_pred_neigh),\n",
    "           metrics.accuracy_score(y_deploy, y_pred_svm_linear), \n",
    "           metrics.accuracy_score(y_deploy, y_pred_svm), \n",
    "           metrics.accuracy_score(y_deploy, y_pred_gtgini),\n",
    "           metrics.accuracy_score(y_deploy, y_pred_dt_IG), \n",
    "           metrics.accuracy_score(y_deploy, y_pred_dt_BGKN),\n",
    "           metrics.accuracy_score(y_deploy, y_pred_dt_BGDT), \n",
    "           metrics.accuracy_score(y_deploy, y_pred_dt_RF),\n",
    "           metrics.accuracy_score(y_deploy, y_pred_dt_AD), \n",
    "           metrics.accuracy_score(y_deploy, y_pred_NB),\n",
    "           metrics.accuracy_score(y_deploy, y_pred_dt_LDA), \n",
    "           metrics.accuracy_score(y_deploy, y_pred_dt_QDA),           \n",
    "           metrics.accuracy_score(y_deploy, y_pred_LR),\n",
    "           metrics.accuracy_score(y_deploy, y_pred_GP),\n",
    "           metrics.accuracy_score(y_deploy, y_pred_gbm),\n",
    "           metrics.accuracy_score(y_deploy, y_pred_xgboost),\n",
    "           metrics.accuracy_score(y_deploy, y_pred_NN),        \n",
    "           metrics.accuracy_score(y_deploy, y_pred_HG),\n",
    "           metrics.accuracy_score(y_deploy, y_pred_EXP)]\n",
    "          \n",
    "PresionData = [metrics.precision_score(y_deploy, y_pred_neigh, average=\"macro\"),\n",
    "               metrics.precision_score(y_deploy, y_pred_svm_linear, average=\"macro\"),\n",
    "               metrics.precision_score(y_deploy, y_pred_svm, average=\"macro\"),\n",
    "               metrics.precision_score(y_deploy, y_pred_gtgini, average=\"macro\"),\n",
    "               metrics.precision_score(y_deploy, y_pred_dt_IG, average=\"macro\"),\n",
    "               metrics.precision_score(y_deploy, y_pred_dt_BGKN, average=\"macro\"),\n",
    "               metrics.precision_score(y_deploy, y_pred_dt_BGDT, average=\"macro\"),\n",
    "               metrics.precision_score(y_deploy, y_pred_dt_RF, average=\"macro\"),\n",
    "               metrics.precision_score(y_deploy, y_pred_dt_AD, average=\"macro\"),\n",
    "               metrics.precision_score(y_deploy, y_pred_NB, average=\"macro\"),\n",
    "               metrics.precision_score(y_deploy, y_pred_dt_LDA, average=\"macro\"),\n",
    "               metrics.precision_score(y_deploy, y_pred_dt_QDA, average=\"macro\"),              \n",
    "               metrics.precision_score(y_deploy, y_pred_LR, average=\"macro\"),\n",
    "               metrics.precision_score(y_deploy, y_pred_GP, average=\"macro\"),\n",
    "               metrics.precision_score(y_deploy, y_pred_gbm, average=\"macro\"),\n",
    "               metrics.precision_score(y_deploy, y_pred_xgboost, average=\"macro\"),\n",
    "               metrics.precision_score(y_deploy, y_pred_NN, average=\"macro\"),\n",
    "               metrics.precision_score(y_deploy, y_pred_HG, average=\"macro\"),\n",
    "               metrics.precision_score(y_deploy, y_pred_EXP, average=\"macro\")]\n",
    "\n",
    "RecallData = [ metrics.recall_score(y_deploy, y_pred_neigh, average=\"macro\"),\n",
    "              metrics.recall_score(y_deploy, y_pred_svm_linear, average=\"macro\"),\n",
    "              metrics.recall_score(y_deploy, y_pred_svm, average=\"macro\"),\n",
    "              metrics.recall_score(y_deploy, y_pred_gtgini, average=\"macro\"),\n",
    "              metrics.recall_score(y_deploy, y_pred_dt_IG, average=\"macro\"),\n",
    "              metrics.recall_score(y_deploy, y_pred_dt_BGKN, average=\"macro\"),\n",
    "              metrics.recall_score(y_deploy, y_pred_dt_BGDT, average=\"macro\"),\n",
    "              metrics.recall_score(y_deploy, y_pred_dt_RF, average=\"macro\"),\n",
    "              metrics.recall_score(y_deploy, y_pred_dt_AD, average=\"macro\"),\n",
    "              metrics.recall_score(y_deploy, y_pred_NB, average=\"macro\"),\n",
    "              metrics.recall_score(y_deploy, y_pred_dt_LDA, average=\"macro\"),\n",
    "              metrics.recall_score(y_deploy, y_pred_dt_QDA, average=\"macro\"),\n",
    "              metrics.recall_score(y_deploy, y_pred_LR, average=\"macro\"),\n",
    "              metrics.recall_score(y_deploy, y_pred_GP, average=\"macro\"),\n",
    "              metrics.recall_score(y_deploy, y_pred_gbm, average=\"macro\"),\n",
    "              metrics.recall_score(y_deploy, y_pred_xgboost, average=\"macro\"),       \n",
    "              metrics.recall_score(y_deploy, y_pred_NN, average=\"macro\"),\n",
    "              metrics.recall_score(y_deploy, y_pred_HG, average=\"macro\"),\n",
    "              metrics.recall_score(y_deploy, y_pred_EXP, average=\"macro\")]\n",
    "\n",
    "F1Data = [metrics.f1_score(y_deploy, y_pred_neigh, average=\"macro\"),\n",
    "          metrics.f1_score(y_deploy, y_pred_svm_linear, average=\"macro\"),\n",
    "          metrics.f1_score(y_deploy, y_pred_svm, average=\"macro\"),\n",
    "          metrics.f1_score(y_deploy, y_pred_gtgini, average=\"macro\"),\n",
    "          metrics.f1_score(y_deploy, y_pred_dt_IG, average=\"macro\"),\n",
    "          metrics.f1_score(y_deploy, y_pred_dt_BGKN, average=\"macro\"),\n",
    "          metrics.f1_score(y_deploy, y_pred_dt_BGDT, average=\"macro\"),\n",
    "          metrics.f1_score(y_deploy, y_pred_dt_RF, average=\"macro\"),\n",
    "          metrics.f1_score(y_deploy, y_pred_dt_AD, average=\"macro\"),\n",
    "          metrics.f1_score(y_deploy, y_pred_NB, average=\"macro\"),\n",
    "          metrics.f1_score(y_deploy, y_pred_dt_LDA, average=\"macro\"),\n",
    "          metrics.f1_score(y_deploy, y_pred_dt_QDA, average=\"macro\"),         \n",
    "          metrics.f1_score(y_deploy, y_pred_LR, average=\"macro\"),\n",
    "          metrics.f1_score(y_deploy, y_pred_GP, average=\"macro\"),\n",
    "          metrics.f1_score(y_deploy, y_pred_gbm, average=\"macro\"),\n",
    "          metrics.f1_score(y_deploy, y_pred_xgboost, average=\"macro\"),\n",
    "          metrics.f1_score(y_deploy, y_pred_NN, average=\"macro\"),                             \n",
    "          metrics.f1_score(y_deploy, y_pred_HG, average=\"macro\"),\n",
    "          metrics.f1_score(y_deploy, y_pred_EXP, average=\"macro\")]         \n",
    "\n",
    "\n",
    "\n",
    "N = len(accData)\n",
    "## necessary variables\n",
    "ind = np.arange(N)                # the x locations for the groups\n",
    "width = 0.17                     # the width of the bars\n",
    "## the bars\n",
    "rects1 = ax.bar(ind, accData, width,\n",
    "                color='black',\n",
    "                #yerr=menStd,\n",
    "                error_kw=dict(elinewidth=2,ecolor='red'))\n",
    "rects2 = ax.bar(ind+width, F1Data, width,\n",
    "                    color='red',\n",
    "                    #yerr=womenStd,\n",
    "                    error_kw=dict(elinewidth=2,ecolor='black'))\n",
    "rects3 = ax.bar(ind+width+width, PresionData, width,\n",
    "                    color='green',\n",
    "                    #yerr=womenStd,\n",
    "                    error_kw=dict(elinewidth=2,ecolor='blue'))\n",
    "rects4 = ax.bar(ind+width+width+width, RecallData, width,\n",
    "                    color='blue',\n",
    "                    #yerr=womenStd,\n",
    "                    error_kw=dict(elinewidth=2,ecolor='green'))\n",
    "# axes and labels\n",
    "ax.set_xlim(-width,len(ind)+width)\n",
    "ax.set_ylim(0,1)\n",
    "ax.set_ylabel('Score')\n",
    "ax.set_xlabel('Name of Classifier')\n",
    "ax.set_title('Scores of different classifiers on Test Data')\n",
    "xTickMarks = ['Knn', 'LSVM', 'SVM', 'DT_gini', 'DT_entorpy' ,\n",
    "              'Bagging Knn' , 'Bagging DT' , 'Random Forest' , 'Ada Boost' ,\n",
    "              'NB' , 'LDA' , 'QDA' ,'Log. Reg.' ,'GP Class.','LightGBM','Xgboost',\n",
    "              'NN', 'Hedge', 'EXP3']\n",
    "ax.set_xticks(ind+width)\n",
    "xtickNames = ax.set_xticklabels(xTickMarks)\n",
    "plt.setp(xtickNames, rotation=90, fontsize=10)\n",
    "## add a legend\n",
    "ax.legend( (rects1[0], rects2[0], rects3[0], rects4[0]), ('Acc.', 'F1' , 'Prec.' , 'Recall') , loc=8, fancybox=True, \n",
    "          frameon=True, shadow=True)\n",
    "ax.set_facecolor('0.9')\n",
    "ax.spines['top'].set_visible(True)\n",
    "ax.spines['right'].set_visible(True)\n",
    "ax.spines['bottom'].set_visible(True)\n",
    "ax.spines['left'].set_visible(True)\n",
    "\n",
    "ax.spines['top'].set_linewidth(0.9)\n",
    "ax.spines['right'].set_linewidth(0.9)\n",
    "ax.spines['bottom'].set_linewidth(0.9)\n",
    "ax.spines['left'].set_linewidth(0.9)\n",
    "ax.grid(False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from math import pi\n",
    "from bokeh.charts import Bar, Area, defaults\n",
    "from bokeh.layouts import row\n",
    "from bokeh.charts.attributes import cat, color\n",
    "from bokeh.charts.operations import blend\n",
    "#from bokeh.charts.utils import df_from_json\n",
    "from bokeh.plotting import figure, output_notebook, show\n",
    "############################################################################################################\n",
    "TOOLS = 'box_zoom,box_select,crosshair,resize,reset,lasso_select,pan,save,poly_select,tap,wheel_zoom,undo'\n",
    "#defaults.width = 1000\n",
    "#defaults.height = 800\n",
    "output_notebook()\n",
    "df1 = pd.DataFrame({'Matric': xTickMarks,\n",
    "                    'Accuracy':accData, \n",
    "                   'Precision': PresionData, \n",
    "                   'Recall': RecallData, \n",
    "                    'F1 Score': F1Data})\n",
    "############################################################################################################\n",
    "bar = Bar(df1,\n",
    "          values=blend('Accuracy', 'F1 Score', 'Precision','Recall', name='Scores', labels_name='Score'),\n",
    "          label=cat(columns='Matric', sort=False),\n",
    "          stack=cat(columns='Score', sort=False),\n",
    "          color=color(columns='Score', palette=['SaddleBrown', 'Silver', 'Goldenrod', 'Grey'],\n",
    "                      sort=False),\n",
    "          legend='bottom_center', xlabel=\"List of Models\", ylabel=\"The Scores\",\n",
    "          title=\"Scores of different Models\", \n",
    "          tooltips=[('Score', '@Score'), ('Model', '@Matric')],\n",
    "          tools=TOOLS, plot_width=900, plot_height=800)\n",
    "bar.title.align = \"center\"\n",
    "bar.xaxis.major_label_orientation = pi/2\n",
    "###############################################################################################################\n",
    "p = Bar(df1, label='Matric', \n",
    "        values = blend('Accuracy', 'F1 Score', 'Precision','Recall', name='Scores', labels_name='Score'),\n",
    "        group=cat(columns='Score', sort=False),\n",
    "        title=\"Scores of different Models\", legend='bottom_center',\n",
    "       tools=TOOLS, plot_width=900, plot_height=600,\n",
    "       xlabel='List of Models', ylabel='The Scores')\n",
    "p.title.align = \"center\"\n",
    "#p.yaxis.major_label_orientation = \"vertical\"\n",
    "p.xaxis.major_label_orientation = pi/2\n",
    "#########################################################################################################\n",
    "data = dict(\n",
    "    Acc = accData,\n",
    "    Pre = PresionData,\n",
    "    Rec = RecallData,\n",
    "    F1 = F1Data,\n",
    ")\n",
    "area1 = Area(data, title=\"The trend of score over Models\", legend=\"bottom_center\",\n",
    "             xlabel='List of Models', ylabel='The Scores',\n",
    "            tools=TOOLS, plot_width=450, plot_height=300)\n",
    "area1.title.align = \"center\"\n",
    "area2 = Area(data, title=\"The trend of score over Models\", legend=\"bottom_center\",\n",
    "             stack=True, xlabel='List of Models', ylabel='The Scores',\n",
    "            tools=TOOLS, plot_width=450, plot_height=300)\n",
    "area2.title.align = \"center\"\n",
    "#########################################################################################################\n",
    "show(bar)\n",
    "show(p)\n",
    "#show(area1)\n",
    "#show(area2)\n",
    "show(row(area1, area2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Thompson Sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "accData = [metrics.accuracy_score(y_deploy, y_pred_neigh),\n",
    "           metrics.accuracy_score(y_deploy, y_pred_svm_linear), \n",
    "           metrics.accuracy_score(y_deploy, y_pred_svm), \n",
    "           metrics.accuracy_score(y_deploy, y_pred_gtgini),\n",
    "           metrics.accuracy_score(y_deploy, y_pred_dt_IG), \n",
    "           metrics.accuracy_score(y_deploy, y_pred_dt_BGKN),\n",
    "           metrics.accuracy_score(y_deploy, y_pred_dt_BGDT), \n",
    "           metrics.accuracy_score(y_deploy, y_pred_dt_RF),\n",
    "           metrics.accuracy_score(y_deploy, y_pred_dt_AD), \n",
    "           metrics.accuracy_score(y_deploy, y_pred_NB),\n",
    "           metrics.accuracy_score(y_deploy, y_pred_dt_LDA), \n",
    "           metrics.accuracy_score(y_deploy, y_pred_dt_QDA),           \n",
    "           metrics.accuracy_score(y_deploy, y_pred_LR),\n",
    "           metrics.accuracy_score(y_deploy, y_pred_GP),\n",
    "           metrics.accuracy_score(y_deploy, y_pred_gbm),\n",
    "           metrics.accuracy_score(y_deploy, y_pred_xgboost),\n",
    "           metrics.accuracy_score(y_deploy, y_pred_NN),\n",
    "           metrics.accuracy_score(y_deploy, y_pred_TS)]\n",
    "          \n",
    "PresionData = [metrics.precision_score(y_deploy, y_pred_neigh, average=\"macro\"),\n",
    "               metrics.precision_score(y_deploy, y_pred_svm_linear, average=\"macro\"),\n",
    "               metrics.precision_score(y_deploy, y_pred_svm, average=\"macro\"),\n",
    "               metrics.precision_score(y_deploy, y_pred_gtgini, average=\"macro\"),\n",
    "               metrics.precision_score(y_deploy, y_pred_dt_IG, average=\"macro\"),\n",
    "               metrics.precision_score(y_deploy, y_pred_dt_BGKN, average=\"macro\"),\n",
    "               metrics.precision_score(y_deploy, y_pred_dt_BGDT, average=\"macro\"),\n",
    "               metrics.precision_score(y_deploy, y_pred_dt_RF, average=\"macro\"),\n",
    "               metrics.precision_score(y_deploy, y_pred_dt_AD, average=\"macro\"),\n",
    "               metrics.precision_score(y_deploy, y_pred_NB, average=\"macro\"),\n",
    "               metrics.precision_score(y_deploy, y_pred_dt_LDA, average=\"macro\"),\n",
    "               metrics.precision_score(y_deploy, y_pred_dt_QDA, average=\"macro\"),              \n",
    "               metrics.precision_score(y_deploy, y_pred_LR, average=\"macro\"),\n",
    "               metrics.precision_score(y_deploy, y_pred_GP, average=\"macro\"),\n",
    "               metrics.precision_score(y_deploy, y_pred_gbm, average=\"macro\"),\n",
    "               metrics.precision_score(y_deploy, y_pred_xgboost, average=\"macro\"),\n",
    "               metrics.precision_score(y_deploy, y_pred_NN, average=\"macro\"),\n",
    "               metrics.precision_score(y_deploy, y_pred_TS, average=\"macro\")]\n",
    "\n",
    "RecallData = [ metrics.recall_score(y_deploy, y_pred_neigh, average=\"macro\"),\n",
    "              metrics.recall_score(y_deploy, y_pred_svm_linear, average=\"macro\"),\n",
    "              metrics.recall_score(y_deploy, y_pred_svm, average=\"macro\"),\n",
    "              metrics.recall_score(y_deploy, y_pred_gtgini, average=\"macro\"),\n",
    "              metrics.recall_score(y_deploy, y_pred_dt_IG, average=\"macro\"),\n",
    "              metrics.recall_score(y_deploy, y_pred_dt_BGKN, average=\"macro\"),\n",
    "              metrics.recall_score(y_deploy, y_pred_dt_BGDT, average=\"macro\"),\n",
    "              metrics.recall_score(y_deploy, y_pred_dt_RF, average=\"macro\"),\n",
    "              metrics.recall_score(y_deploy, y_pred_dt_AD, average=\"macro\"),\n",
    "              metrics.recall_score(y_deploy, y_pred_NB, average=\"macro\"),\n",
    "              metrics.recall_score(y_deploy, y_pred_dt_LDA, average=\"macro\"),\n",
    "              metrics.recall_score(y_deploy, y_pred_dt_QDA, average=\"macro\"),\n",
    "              metrics.recall_score(y_deploy, y_pred_LR, average=\"macro\"),\n",
    "              metrics.recall_score(y_deploy, y_pred_GP, average=\"macro\"),\n",
    "              metrics.recall_score(y_deploy, y_pred_gbm, average=\"macro\"),\n",
    "              metrics.recall_score(y_deploy, y_pred_xgboost, average=\"macro\"),       \n",
    "              metrics.recall_score(y_deploy, y_pred_NN, average=\"macro\"),\n",
    "              metrics.recall_score(y_deploy, y_pred_TS, average=\"macro\")]\n",
    "\n",
    "F1Data = [metrics.f1_score(y_deploy, y_pred_neigh, average=\"macro\"),\n",
    "          metrics.f1_score(y_deploy, y_pred_svm_linear, average=\"macro\"),\n",
    "          metrics.f1_score(y_deploy, y_pred_svm, average=\"macro\"),\n",
    "          metrics.f1_score(y_deploy, y_pred_gtgini, average=\"macro\"),\n",
    "          metrics.f1_score(y_deploy, y_pred_dt_IG, average=\"macro\"),\n",
    "          metrics.f1_score(y_deploy, y_pred_dt_BGKN, average=\"macro\"),\n",
    "          metrics.f1_score(y_deploy, y_pred_dt_BGDT, average=\"macro\"),\n",
    "          metrics.f1_score(y_deploy, y_pred_dt_RF, average=\"macro\"),\n",
    "          metrics.f1_score(y_deploy, y_pred_dt_AD, average=\"macro\"),\n",
    "          metrics.f1_score(y_deploy, y_pred_NB, average=\"macro\"),\n",
    "          metrics.f1_score(y_deploy, y_pred_dt_LDA, average=\"macro\"),\n",
    "          metrics.f1_score(y_deploy, y_pred_dt_QDA, average=\"macro\"),         \n",
    "          metrics.f1_score(y_deploy, y_pred_LR, average=\"macro\"),\n",
    "          metrics.f1_score(y_deploy, y_pred_GP, average=\"macro\"),\n",
    "          metrics.f1_score(y_deploy, y_pred_gbm, average=\"macro\"),\n",
    "          metrics.f1_score(y_deploy, y_pred_xgboost, average=\"macro\"),\n",
    "          metrics.f1_score(y_deploy, y_pred_NN, average=\"macro\"),                             \n",
    "          metrics.f1_score(y_deploy, y_pred_TS, average=\"macro\")]         \n",
    "\n",
    "\n",
    "\n",
    "N = len(accData)\n",
    "## necessary variables\n",
    "ind = np.arange(N)                # the x locations for the groups\n",
    "width = 0.17                     # the width of the bars\n",
    "## the bars\n",
    "rects1 = ax.bar(ind, accData, width,\n",
    "                color='black',\n",
    "                #yerr=menStd,\n",
    "                error_kw=dict(elinewidth=2,ecolor='red'))\n",
    "rects2 = ax.bar(ind+width, F1Data, width,\n",
    "                    color='red',\n",
    "                    #yerr=womenStd,\n",
    "                    error_kw=dict(elinewidth=2,ecolor='black'))\n",
    "rects3 = ax.bar(ind+width+width, PresionData, width,\n",
    "                    color='green',\n",
    "                    #yerr=womenStd,\n",
    "                    error_kw=dict(elinewidth=2,ecolor='blue'))\n",
    "rects4 = ax.bar(ind+width+width+width, RecallData, width,\n",
    "                    color='blue',\n",
    "                    #yerr=womenStd,\n",
    "                    error_kw=dict(elinewidth=2,ecolor='green'))\n",
    "# axes and labels\n",
    "ax.set_xlim(-width,len(ind)+width)\n",
    "ax.set_ylim(0,1)\n",
    "ax.set_ylabel('Score')\n",
    "ax.set_xlabel('Name of Classifier')\n",
    "ax.set_title('Scores of different classifiers on Test Data')\n",
    "xTickMarks = ['Knn', 'LSVM', 'SVM', 'DT_gini', 'DT_entorpy' ,\n",
    "              'Bagging Knn' , 'Bagging DT' , 'Random Forest' , 'Ada Boost' ,\n",
    "              'NB' , 'LDA' , 'QDA' ,'Log. Reg.' ,'GP Class.','LightGBM','Xgboost',\n",
    "              'NN', 'Tomp. Sampling']\n",
    "ax.set_xticks(ind+width)\n",
    "xtickNames = ax.set_xticklabels(xTickMarks)\n",
    "plt.setp(xtickNames, rotation=90, fontsize=10, )\n",
    "## add a legend\n",
    "ax.legend( (rects1[0], rects2[0], rects3[0], rects4[0]), ('Acc.', 'F1' , 'Prec.' , 'Recall') , loc=8, fancybox=True, \n",
    "          frameon=True, shadow=True)\n",
    "ax.set_facecolor('0.9')\n",
    "\n",
    "ax.spines['top'].set_visible(True)\n",
    "ax.spines['right'].set_visible(True)\n",
    "ax.spines['bottom'].set_visible(True)\n",
    "ax.spines['left'].set_visible(True)\n",
    "\n",
    "ax.spines['top'].set_linewidth(0.9)\n",
    "ax.spines['right'].set_linewidth(0.9)\n",
    "ax.spines['bottom'].set_linewidth(0.9)\n",
    "ax.spines['left'].set_linewidth(0.9)\n",
    "ax.grid(False)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from math import pi\n",
    "from bokeh.charts import Bar, Area, defaults\n",
    "from bokeh.layouts import row\n",
    "from bokeh.charts.attributes import cat, color\n",
    "from bokeh.charts.operations import blend\n",
    "#from bokeh.charts.utils import df_from_json\n",
    "from bokeh.plotting import figure, output_notebook, show\n",
    "############################################################################################################\n",
    "TOOLS = 'box_zoom,box_select,crosshair,resize,reset,lasso_select,pan,save,poly_select,tap,wheel_zoom,undo'\n",
    "#defaults.width = 1000\n",
    "#defaults.height = 800\n",
    "output_notebook()\n",
    "df1 = pd.DataFrame({'Matric': xTickMarks,\n",
    "                    'Accuracy':accData, \n",
    "                   'Precision': PresionData, \n",
    "                   'Recall': RecallData, \n",
    "                    'F1 Score': F1Data})\n",
    "############################################################################################################\n",
    "bar = Bar(df1,\n",
    "          values=blend('Accuracy', 'F1 Score', 'Precision','Recall', name='Scores', labels_name='Score'),\n",
    "          label=cat(columns='Matric', sort=False),\n",
    "          stack=cat(columns='Score', sort=False),\n",
    "          color=color(columns='Score', palette=['SaddleBrown', 'Silver', 'Goldenrod', 'Grey'],\n",
    "                      sort=False),\n",
    "          legend='bottom_center', xlabel=\"List of Models\", ylabel=\"The Scores\",\n",
    "          title=\"Scores of different Models\", \n",
    "          tooltips=[('Score', '@Score'), ('Model', '@Matric')],\n",
    "          tools=TOOLS, plot_width=900, plot_height=800)\n",
    "bar.title.align = \"center\"\n",
    "bar.xaxis.major_label_orientation = pi/2\n",
    "###############################################################################################################\n",
    "p = Bar(df1, label='Matric', \n",
    "        values = blend('Accuracy', 'F1 Score', 'Precision','Recall', name='Scores', labels_name='Score'),\n",
    "        group=cat(columns='Score', sort=False),\n",
    "        title=\"Scores of different Models\", legend='bottom_center',\n",
    "       tools=TOOLS, plot_width=900, plot_height=600,\n",
    "       xlabel='List of Models', ylabel='The Scores')\n",
    "p.title.align = \"center\"\n",
    "#p.yaxis.major_label_orientation = \"vertical\"\n",
    "p.xaxis.major_label_orientation = pi/2\n",
    "#########################################################################################################\n",
    "data = dict(\n",
    "    Acc = accData,\n",
    "    Pre = PresionData,\n",
    "    Rec = RecallData,\n",
    "    F1 = F1Data,\n",
    ")\n",
    "area1 = Area(data, title=\"The trend of score over Models\", legend=\"bottom_center\",\n",
    "             xlabel='List of Models', ylabel='The Scores',\n",
    "            tools=TOOLS, plot_width=450, plot_height=300)\n",
    "area1.title.align = \"center\"\n",
    "area2 = Area(data, title=\"The trend of score over Models\", legend=\"bottom_center\",\n",
    "             stack=True, xlabel='List of Models', ylabel='The Scores',\n",
    "            tools=TOOLS, plot_width=450, plot_height=300)\n",
    "area2.title.align = \"center\"\n",
    "#########################################################################################################\n",
    "show(bar)\n",
    "show(p)\n",
    "#show(area1)\n",
    "#show(area2)\n",
    "show(row(area1, area2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compute Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "def plot_confusion_matrix(cm, title='Confusion matrix', cmap=plt.cm.Blues):\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(targetLabel))\n",
    "    plt.xticks(tick_marks, targetLabel)\n",
    "    plt.yticks(tick_marks, targetLabel)\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "targetLabel = np.array(['1','2','3','4','5','6','7'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute Confusion on Knn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cm = confusion_matrix(y_deploy, y_pred_neigh)\n",
    "np.set_printoptions(precision=2)\n",
    "print('Confusion matrix, without normalization')\n",
    "print(cm)\n",
    "plt.figure(1)\n",
    "plt.subplot(2,2,1)\n",
    "\n",
    "plot_confusion_matrix(cm)\n",
    "\n",
    "# Normalize the confusion matrix by row (i.e by the number of samples\n",
    "# in each class)\n",
    "cm_normalized = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "print('Normalized confusion matrix')\n",
    "print(cm_normalized)\n",
    "#plt.figure()\n",
    "plt.subplot(2,2,2)\n",
    "\n",
    "plot_confusion_matrix(cm_normalized, title='Normalized confusion matrix')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Compute Confusion on LSVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cm = confusion_matrix(y_deploy, y_pred_svm_linear)\n",
    "np.set_printoptions(precision=2)\n",
    "print('Confusion matrix, without normalization')\n",
    "print(cm)\n",
    "plt.figure(1)\n",
    "plt.subplot(2,2,1)\n",
    "\n",
    "plot_confusion_matrix(cm)\n",
    "\n",
    "# Normalize the confusion matrix by row (i.e by the number of samples\n",
    "# in each class)\n",
    "cm_normalized = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "print('Normalized confusion matrix')\n",
    "print(cm_normalized)\n",
    "#plt.figure()\n",
    "plt.subplot(2,2,2)\n",
    "\n",
    "plot_confusion_matrix(cm_normalized, title='Normalized confusion matrix')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute Confusion on SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cm = confusion_matrix(y_deploy, y_pred_svm)\n",
    "np.set_printoptions(precision=2)\n",
    "print('Confusion matrix, without normalization')\n",
    "print(cm)\n",
    "plt.figure(1)\n",
    "plt.subplot(2,2,1)\n",
    "\n",
    "plot_confusion_matrix(cm)\n",
    "\n",
    "# Normalize the confusion matrix by row (i.e by the number of samples\n",
    "# in each class)\n",
    "cm_normalized = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "print('Normalized confusion matrix')\n",
    "print(cm_normalized)\n",
    "#plt.figure()\n",
    "plt.subplot(2,2,2)\n",
    "\n",
    "plot_confusion_matrix(cm_normalized, title='Normalized confusion matrix')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute Confusion on DT with gini \"CART\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cm = confusion_matrix(y_deploy, y_pred_gtgini)\n",
    "np.set_printoptions(precision=2)\n",
    "print('Confusion matrix, without normalization')\n",
    "print(cm)\n",
    "plt.figure(1)\n",
    "plt.subplot(2,2,1)\n",
    "\n",
    "plot_confusion_matrix(cm)\n",
    "\n",
    "# Normalize the confusion matrix by row (i.e by the number of samples\n",
    "# in each class)\n",
    "cm_normalized = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "print('Normalized confusion matrix')\n",
    "print(cm_normalized)\n",
    "#plt.figure()\n",
    "plt.subplot(2,2,2)\n",
    "\n",
    "plot_confusion_matrix(cm_normalized, title='Normalized confusion matrix')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute Confusion on DT with entory \"C5.0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cm = confusion_matrix(y_deploy, y_pred_dt_IG)\n",
    "np.set_printoptions(precision=2)\n",
    "print('Confusion matrix, without normalization')\n",
    "print(cm)\n",
    "plt.figure(1)\n",
    "plt.subplot(2,2,1)\n",
    "\n",
    "plot_confusion_matrix(cm)\n",
    "\n",
    "# Normalize the confusion matrix by row (i.e by the number of samples\n",
    "# in each class)\n",
    "cm_normalized = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "print('Normalized confusion matrix')\n",
    "print(cm_normalized)\n",
    "#plt.figure()\n",
    "plt.subplot(2,2,2)\n",
    "\n",
    "plot_confusion_matrix(cm_normalized, title='Normalized confusion matrix')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute Confusion on Bagging with Knn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cm = confusion_matrix(y_deploy, y_pred_dt_BGKN)\n",
    "np.set_printoptions(precision=2)\n",
    "print('Confusion matrix, without normalization')\n",
    "print(cm)\n",
    "plt.figure(1)\n",
    "plt.subplot(2,2,1)\n",
    "\n",
    "plot_confusion_matrix(cm)\n",
    "\n",
    "# Normalize the confusion matrix by row (i.e by the number of samples\n",
    "# in each class)\n",
    "cm_normalized = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "print('Normalized confusion matrix')\n",
    "print(cm_normalized)\n",
    "#plt.figure()\n",
    "plt.subplot(2,2,2)\n",
    "\n",
    "plot_confusion_matrix(cm_normalized, title='Normalized confusion matrix')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute Confusion on Bagging with DT \"CART\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cm = confusion_matrix(y_deploy, y_pred_dt_BGDT)\n",
    "np.set_printoptions(precision=2)\n",
    "print('Confusion matrix, without normalization')\n",
    "print(cm)\n",
    "plt.figure(1)\n",
    "plt.subplot(2,2,1)\n",
    "\n",
    "plot_confusion_matrix(cm)\n",
    "\n",
    "# Normalize the confusion matrix by row (i.e by the number of samples\n",
    "# in each class)\n",
    "cm_normalized = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "print('Normalized confusion matrix')\n",
    "print(cm_normalized)\n",
    "#plt.figure()\n",
    "plt.subplot(2,2,2)\n",
    "\n",
    "plot_confusion_matrix(cm_normalized, title='Normalized confusion matrix')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute Confusion on Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cm = confusion_matrix(y_deploy, y_pred_dt_RF)\n",
    "np.set_printoptions(precision=2)\n",
    "print('Confusion matrix, without normalization')\n",
    "print(cm)\n",
    "plt.figure(1)\n",
    "plt.subplot(2,2,1)\n",
    "\n",
    "plot_confusion_matrix(cm)\n",
    "\n",
    "# Normalize the confusion matrix by row (i.e by the number of samples\n",
    "# in each class)\n",
    "cm_normalized = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "print('Normalized confusion matrix')\n",
    "print(cm_normalized)\n",
    "#plt.figure()\n",
    "plt.subplot(2,2,2)\n",
    "\n",
    "plot_confusion_matrix(cm_normalized, title='Normalized confusion matrix')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute Confusion on Ada Boost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cm = confusion_matrix(y_deploy, y_pred_dt_AD)\n",
    "np.set_printoptions(precision=2)\n",
    "print('Confusion matrix, without normalization')\n",
    "print(cm)\n",
    "plt.figure(1)\n",
    "plt.subplot(2,2,1)\n",
    "\n",
    "plot_confusion_matrix(cm)\n",
    "\n",
    "# Normalize the confusion matrix by row (i.e by the number of samples\n",
    "# in each class)\n",
    "cm_normalized = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "print('Normalized confusion matrix')\n",
    "print(cm_normalized)\n",
    "#plt.figure()\n",
    "plt.subplot(2,2,2)\n",
    "\n",
    "plot_confusion_matrix(cm_normalized, title='Normalized confusion matrix')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute Confusion on Naive Bayes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cm = confusion_matrix(y_deploy, y_pred_NB)\n",
    "np.set_printoptions(precision=2)\n",
    "print('Confusion matrix, without normalization')\n",
    "print(cm)\n",
    "plt.figure(1)\n",
    "plt.subplot(2,2,1)\n",
    "\n",
    "plot_confusion_matrix(cm)\n",
    "\n",
    "# Normalize the confusion matrix by row (i.e by the number of samples\n",
    "# in each class)\n",
    "cm_normalized = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "print('Normalized confusion matrix')\n",
    "print(cm_normalized)\n",
    "#plt.figure()\n",
    "plt.subplot(2,2,2)\n",
    "\n",
    "plot_confusion_matrix(cm_normalized, title='Normalized confusion matrix')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute Confusion on LDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cm = confusion_matrix(y_deploy, y_pred_dt_LDA)\n",
    "np.set_printoptions(precision=2)\n",
    "print('Confusion matrix, without normalization')\n",
    "print(cm)\n",
    "plt.figure(1)\n",
    "plt.subplot(2,2,1)\n",
    "\n",
    "plot_confusion_matrix(cm)\n",
    "\n",
    "# Normalize the confusion matrix by row (i.e by the number of samples\n",
    "# in each class)\n",
    "cm_normalized = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "print('Normalized confusion matrix')\n",
    "print(cm_normalized)\n",
    "#plt.figure()\n",
    "plt.subplot(2,2,2)\n",
    "\n",
    "plot_confusion_matrix(cm_normalized, title='Normalized confusion matrix')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute Confusion on QDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cm = confusion_matrix(y_deploy, y_pred_dt_QDA)\n",
    "np.set_printoptions(precision=2)\n",
    "print('Confusion matrix, without normalization')\n",
    "print(cm)\n",
    "plt.figure(1)\n",
    "plt.subplot(2,2,1)\n",
    "\n",
    "plot_confusion_matrix(cm)\n",
    "\n",
    "# Normalize the confusion matrix by row (i.e by the number of samples\n",
    "# in each class)\n",
    "cm_normalized = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "print('Normalized confusion matrix')\n",
    "print(cm_normalized)\n",
    "#plt.figure()\n",
    "plt.subplot(2,2,2)\n",
    "\n",
    "plot_confusion_matrix(cm_normalized, title='Normalized confusion matrix')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute Confusion on NN\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cm = confusion_matrix(y_deploy, y_pred_NN)\n",
    "np.set_printoptions(precision=2)\n",
    "print('Confusion matrix, without normalization')\n",
    "print(cm)\n",
    "plt.figure(1)\n",
    "plt.subplot(2,2,1)\n",
    "\n",
    "plot_confusion_matrix(cm)\n",
    "\n",
    "# Normalize the confusion matrix by row (i.e by the number of samples\n",
    "# in each class)\n",
    "cm_normalized = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "print('Normalized confusion matrix')\n",
    "print(cm_normalized)\n",
    "#plt.figure()\n",
    "plt.subplot(2,2,2)\n",
    "\n",
    "plot_confusion_matrix(cm_normalized, title='Normalized confusion matrix')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute Confusion on UCB1 NN 2.5% neurals removed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cm = confusion_matrix(y_deploy, y_pred_NN1)\n",
    "np.set_printoptions(precision=2)\n",
    "print('Confusion matrix, without normalization')\n",
    "print(cm)\n",
    "plt.figure(1)\n",
    "plt.subplot(2,2,1)\n",
    "\n",
    "plot_confusion_matrix(cm)\n",
    "\n",
    "# Normalize the confusion matrix by row (i.e by the number of samples\n",
    "# in each class)\n",
    "cm_normalized = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "print('Normalized confusion matrix')\n",
    "print(cm_normalized)\n",
    "#plt.figure()\n",
    "plt.subplot(2,2,2)\n",
    "\n",
    "plot_confusion_matrix(cm_normalized, title='Normalized confusion matrix')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute Confusion on NN 7.5% neurals removed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cm = confusion_matrix(y_deploy, y_pred_NN2)\n",
    "np.set_printoptions(precision=2)\n",
    "print('Confusion matrix, without normalization')\n",
    "print(cm)\n",
    "plt.figure(1)\n",
    "plt.subplot(2,2,1)\n",
    "\n",
    "plot_confusion_matrix(cm)\n",
    "\n",
    "# Normalize the confusion matrix by row (i.e by the number of samples\n",
    "# in each class)\n",
    "cm_normalized = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "print('Normalized confusion matrix')\n",
    "print(cm_normalized)\n",
    "#plt.figure()\n",
    "plt.subplot(2,2,2)\n",
    "\n",
    "plot_confusion_matrix(cm_normalized, title='Normalized confusion matrix')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute Confusion on NN 65% neurals removed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cm = confusion_matrix(y_deploy, y_pred_NN3)\n",
    "np.set_printoptions(precision=2)\n",
    "print('Confusion matrix, without normalization')\n",
    "print(cm)\n",
    "plt.figure(1)\n",
    "plt.subplot(2,2,1)\n",
    "\n",
    "plot_confusion_matrix(cm)\n",
    "\n",
    "# Normalize the confusion matrix by row (i.e by the number of samples\n",
    "# in each class)\n",
    "cm_normalized = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "print('Normalized confusion matrix')\n",
    "print(cm_normalized)\n",
    "#plt.figure()\n",
    "plt.subplot(2,2,2)\n",
    "\n",
    "plot_confusion_matrix(cm_normalized, title='Normalized confusion matrix')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute Confusion on NN 85% neurals removed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cm = confusion_matrix(y_deploy, y_pred_NN4)\n",
    "np.set_printoptions(precision=2)\n",
    "print('Confusion matrix, without normalization')\n",
    "print(cm)\n",
    "plt.figure(1)\n",
    "plt.subplot(2,2,1)\n",
    "\n",
    "plot_confusion_matrix(cm)\n",
    "\n",
    "# Normalize the confusion matrix by row (i.e by the number of samples\n",
    "# in each class)\n",
    "cm_normalized = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "print('Normalized confusion matrix')\n",
    "print(cm_normalized)\n",
    "#plt.figure()\n",
    "plt.subplot(2,2,2)\n",
    "\n",
    "plot_confusion_matrix(cm_normalized, title='Normalized confusion matrix')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute Confusion on Epsilon Greedy neurals removed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cm = confusion_matrix(y_deploy, y_pred_EG)\n",
    "np.set_printoptions(precision=2)\n",
    "print('Confusion matrix, without normalization')\n",
    "print(cm)\n",
    "plt.figure(1)\n",
    "plt.subplot(2,2,1)\n",
    "\n",
    "plot_confusion_matrix(cm)\n",
    "\n",
    "# Normalize the confusion matrix by row (i.e by the number of samples\n",
    "# in each class)\n",
    "cm_normalized = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "print('Normalized confusion matrix')\n",
    "print(cm_normalized)\n",
    "#plt.figure()\n",
    "plt.subplot(2,2,2)\n",
    "\n",
    "plot_confusion_matrix(cm_normalized, title='Normalized confusion matrix')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute Confusion on Decaying Epsilon Greedy neurals removed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cm = confusion_matrix(y_deploy, y_pred_AEG)\n",
    "np.set_printoptions(precision=2)\n",
    "print('Confusion matrix, without normalization')\n",
    "print(cm)\n",
    "plt.figure(1)\n",
    "plt.subplot(2,2,1)\n",
    "plot_confusion_matrix(cm)\n",
    "# Normalize the confusion matrix by row (i.e by the number of samples\n",
    "# in each class)\n",
    "cm_normalized = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "print('Normalized confusion matrix')\n",
    "print(cm_normalized)\n",
    "#plt.figure()\n",
    "plt.subplot(2,2,2)\n",
    "plot_confusion_matrix(cm_normalized, title='Normalized confusion matrix')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute Confusion on SOFTMAX neurals removed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cm = confusion_matrix(y_deploy, y_pred_SM)\n",
    "np.set_printoptions(precision=2)\n",
    "print('Confusion matrix, without normalization')\n",
    "print(cm)\n",
    "plt.figure(1)\n",
    "plt.subplot(2,2,1)\n",
    "plot_confusion_matrix(cm)\n",
    "# Normalize the confusion matrix by row (i.e by the number of samples\n",
    "# in each class)\n",
    "cm_normalized = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "print('Normalized confusion matrix')\n",
    "print(cm_normalized)\n",
    "#plt.figure()\n",
    "plt.subplot(2,2,2)\n",
    "plot_confusion_matrix(cm_normalized, title='Normalized confusion matrix')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute Confusion on DECAYING SOFTMAX neurals removed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cm = confusion_matrix(y_deploy, y_pred_ASM)\n",
    "np.set_printoptions(precision=2)\n",
    "print('Confusion matrix, without normalization')\n",
    "print(cm)\n",
    "plt.figure(1)\n",
    "plt.subplot(2,2,1)\n",
    "plot_confusion_matrix(cm)\n",
    "# Normalize the confusion matrix by row (i.e by the number of samples\n",
    "# in each class)\n",
    "cm_normalized = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "print('Normalized confusion matrix')\n",
    "print(cm_normalized)\n",
    "#plt.figure()\n",
    "plt.subplot(2,2,2)\n",
    "plot_confusion_matrix(cm_normalized, title='Normalized confusion matrix')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute Confusion on THOMPSON SAMPLING neurals removed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cm = confusion_matrix(y_deploy, y_pred_TS)\n",
    "np.set_printoptions(precision=2)\n",
    "print('Confusion matrix, without normalization')\n",
    "print(cm)\n",
    "plt.figure(1)\n",
    "plt.subplot(2,2,1)\n",
    "plot_confusion_matrix(cm)\n",
    "# Normalize the confusion matrix by row (i.e by the number of samples\n",
    "# in each class)\n",
    "cm_normalized = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "print('Normalized confusion matrix')\n",
    "print(cm_normalized)\n",
    "#plt.figure()\n",
    "plt.subplot(2,2,2)\n",
    "plot_confusion_matrix(cm_normalized, title='Normalized confusion matrix')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute Confusion on HEDGE neurals removed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cm = confusion_matrix(y_deploy, y_pred_HG)\n",
    "np.set_printoptions(precision=2)\n",
    "print('Confusion matrix, without normalization')\n",
    "print(cm)\n",
    "plt.figure(1)\n",
    "plt.subplot(2,2,1)\n",
    "plot_confusion_matrix(cm)\n",
    "# Normalize the confusion matrix by row (i.e by the number of samples\n",
    "# in each class)\n",
    "cm_normalized = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "print('Normalized confusion matrix')\n",
    "print(cm_normalized)\n",
    "#plt.figure()\n",
    "plt.subplot(2,2,2)\n",
    "plot_confusion_matrix(cm_normalized, title='Normalized confusion matrix')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute Confusion on EXP3 neurals removed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cm = confusion_matrix(y_deploy, y_pred_EXP)\n",
    "np.set_printoptions(precision=2)\n",
    "print('Confusion matrix, without normalization')\n",
    "print(cm)\n",
    "plt.figure(1)\n",
    "plt.subplot(2,2,1)\n",
    "plot_confusion_matrix(cm)\n",
    "# Normalize the confusion matrix by row (i.e by the number of samples\n",
    "# in each class)\n",
    "cm_normalized = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "print('Normalized confusion matrix')\n",
    "print(cm_normalized)\n",
    "#plt.figure()\n",
    "plt.subplot(2,2,2)\n",
    "plot_confusion_matrix(cm_normalized, title='Normalized confusion matrix')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
