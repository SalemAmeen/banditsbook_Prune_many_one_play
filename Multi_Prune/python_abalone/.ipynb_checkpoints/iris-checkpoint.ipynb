{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas\n",
    "import numpy\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "from pandas.tools.plotting import scatter_matrix\n",
    "import numpy as np\n",
    "import time\n",
    "#from sklearn import metrics\n",
    "from sklearn import cross_validation\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.svm import SVC\n",
    "# fix random seed for reproducibility\n",
    "plt.rcParams['figure.figsize'] = (15, 6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# fix random seed for reproducibility\n",
    "seed = 7\n",
    "numpy.random.seed(seed)\n",
    "url = \"https://archive.ics.uci.edu/ml/machine-learning-databases/iris/iris.data\"\n",
    "names = ['sepal-length', 'sepal-width', 'petal-length', 'petal-width', 'class']\n",
    "data = pandas.read_csv(url, names=names)\n",
    "peek = data.head(10)\n",
    "print(peek)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "shape = data.shape\n",
    "print(shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "types = data.dtypes\n",
    "print(types)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pandas.set_option('display.width', 100)\n",
    "pandas.set_option('precision', 3)\n",
    "description = data.describe()\n",
    "print(description)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class_counts = data.groupby('class').size()\n",
    "print(class_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "correlations = data.corr(method='pearson')\n",
    "print(correlations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "skew = data.skew()\n",
    "print(skew)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visulize the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data.hist()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data.plot(kind='density', subplots=True, layout=(3,3), sharex=False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data.plot(kind='box', subplots=True, layout=(3,3), sharex=False, sharey=False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "correlations = data.corr()\n",
    "# plot correlation matrix\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "cax = ax.matshow(correlations, vmin=-1, vmax=1)\n",
    "fig.colorbar(cax)\n",
    "ticks = numpy.arange(0,4,1)\n",
    "ax.set_xticks(ticks)\n",
    "ax.set_yticks(ticks)\n",
    "ax.set_xticklabels(names)\n",
    "ax.set_yticklabels(names)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "scatter_matrix(data)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare the data for classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "iris = datasets.load_iris()\n",
    "features = iris.data\n",
    "target = iris.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.cross_validation import train_test_split\n",
    "# Extracting 20% testing data\n",
    "X_train_feature, X_deploy, y_train_feature, y_deploy = train_test_split(\n",
    "    features, target, test_size=0.20, random_state=17)\n",
    "# Make training and validation data sets for building the models and choose the hyperparameters\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_train_feature, y_train_feature, test_size=0.20, random_state=17)\n",
    "print 'Number of training examples',len(X_train)\n",
    "print 'Number of validation examples',len(X_test)\n",
    "print 'Number of testing examples',len(X_deploy)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building classifiers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## KNN classifier\n",
    "It turns out that k=5 is the best choose of k on the validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "clf_neigh = KNeighborsClassifier(n_neighbors=5)\n",
    "#Training\n",
    "start_time = time.time()\n",
    "clf_neigh.fit(X_train, y_train)\n",
    "print(\"The time for training KNN is  %s seconds \" % (time.time() - start_time))\n",
    "\n",
    "# Make a prediction\n",
    "y_pred = clf_neigh.predict(X_test)\n",
    "\n",
    "print \"===================================================================\"\n",
    "print \"The accuracy on validation dataset of KNN: \\t\", metrics.accuracy_score(y_test, y_pred)\n",
    "print \"Precision on validation dataset of KNN:    \\t\", metrics.precision_score(y_test, y_pred)\n",
    "print \"Recall on validation dataset of KNN :      \\t\", metrics.recall_score(y_test, y_pred)\n",
    "print \"F1 score on validation dataset of KNN:     \\t\", metrics.f1_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SVM classifier\n",
    "### Linear SVM \"LinearSVC\"\n",
    "It turns out that C=20 is the best on the validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.svm import LinearSVC\n",
    "clf_svm_linear = LinearSVC(C=20.0)\n",
    "#Training\n",
    "start_time = time.time()\n",
    "clf_svm_linear.fit(X_train, y_train)\n",
    "print(\"The time for training SVM is  %s seconds \" % (time.time() - start_time))\n",
    "\n",
    "# Make a prediction\n",
    "y_pred = clf_svm_linear.predict(X_test)\n",
    "\n",
    "print \"===================================================================\"\n",
    "print \"The accuracy on validation dataset of Linear SVM: \\t\", metrics.accuracy_score(y_test, y_pred)\n",
    "print \"Precision on validation dataset of Linear SVM:    \\t\", metrics.precision_score(y_test, y_pred)\n",
    "print \"Recall on validation dataset of Linear SVM :      \\t\", metrics.recall_score(y_test, y_pred)\n",
    "print \"F1 score on validation dataset of Linear SVM:     \\t\", metrics.f1_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  SVM \"SVC\" with kernel='rbf' \n",
    "It turns out that C=5 is the best on the validation set\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "clf_svm = SVC(C=5.0, kernel='rbf')\n",
    "#Training\n",
    "start_time = time.time()\n",
    "clf_svm.fit(X_train, y_train)\n",
    "print(\"The time for training SVM is  %s seconds \" % (time.time() - start_time))\n",
    "\n",
    "# Make a prediction\n",
    "y_pred = clf_svm.predict(X_test)\n",
    "\n",
    "print \"===================================================================\"\n",
    "print \"The accuracy on validation dataset of SVM: \\t\", metrics.accuracy_score(y_test, y_pred)\n",
    "print \"Precision on validation dataset of SVM:    \\t\", metrics.precision_score(y_test, y_pred)\n",
    "print \"Recall on validation dataset of SVM :      \\t\", metrics.recall_score(y_test, y_pred)\n",
    "print \"F1 score on validation dataset of SVM:     \\t\", metrics.f1_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decsion Tree classifier\n",
    "### DT with Gini impurity \"gini\", CART (Classification and Regression Trees)\n",
    "It turns out that min_samples_split=2 is the best on the validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# CART tree\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "clf_dt = DecisionTreeClassifier(min_samples_split=2)\n",
    "#Training\n",
    "start_time = time.time()\n",
    "clf_dt.fit(X_train, y_train)\n",
    "print(\"The time for training Decision Tree is  %s seconds \" % (time.time() - start_time))\n",
    "\n",
    "# Make a prediction\n",
    "y_pred = clf_dt.predict(X_test)\n",
    "\n",
    "print \"===================================================================\"\n",
    "print \"The accuracy on validation dataset of Decision Tree: \\t\", metrics.accuracy_score(y_test, y_pred)\n",
    "print \"Precision on validation dataset of Decision Tree:    \\t\", metrics.precision_score(y_test, y_pred)\n",
    "print \"Recall on validation dataset of Decision Tree :      \\t\", metrics.recall_score(y_test, y_pred)\n",
    "print \"F1 score on validation dataset of Decision Tree:     \\t\", metrics.f1_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DT with “entropy” for the information gain, C4.5 (aka J48 is an open source Java Weka) and C5.0\n",
    "It turns out that min_samples_split=2 is the best on the validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Here I use C5.0 tree as its more accurate that C4.5 and more recent.\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "clf_dt_IG = DecisionTreeClassifier(criterion='entropy', min_samples_split=2)\n",
    "#Training\n",
    "start_time = time.time()\n",
    "clf_dt_IG.fit(X_train, y_train)\n",
    "print(\"The time for training Decision Tree is  %s seconds \" % (time.time() - start_time))\n",
    "\n",
    "# Make a prediction\n",
    "y_pred = clf_dt_IG.predict(X_test)\n",
    "\n",
    "print \"===================================================================\"\n",
    "print \"The accuracy on validation dataset of Decision Tree: \\t\", metrics.accuracy_score(y_test, y_pred)\n",
    "print \"Precision on validation dataset of Decision Tree:    \\t\", metrics.precision_score(y_test, y_pred)\n",
    "print \"Recall on validation dataset of Decision Tree :      \\t\", metrics.recall_score(y_test, y_pred)\n",
    "print \"F1 score on validation dataset of Decision Tree:     \\t\", metrics.f1_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Ensemble methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bagging Method\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bagging Method with Knn\n",
    "\n",
    "\n",
    "Using Best KNN classifier fitted the data, each built on random subsets of 50% of the samples and 50% of the features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "clf_dt_BGKN = BaggingClassifier(KNeighborsClassifier(n_neighbors=5),\n",
    "                             max_samples=0.5, max_features=0.5)\n",
    "start_time = time.time()\n",
    "clf_dt_BGKN.fit(X_train, y_train)\n",
    "print(\"The time for training Bagging Knn is  %s seconds \" % (time.time() - start_time))\n",
    "\n",
    "# Make a prediction\n",
    "y_pred = clf_dt_BGKN.predict(X_test)\n",
    "\n",
    "print \"===================================================================\"\n",
    "print \"The accuracy on validation dataset of Bagging Knn: \\t\", metrics.accuracy_score(y_test, y_pred)\n",
    "print \"Precision on validation dataset of Bagging Knn:    \\t\", metrics.precision_score(y_test, y_pred)\n",
    "print \"Recall on validation dataset of Bagging Knn :      \\t\", metrics.recall_score(y_test, y_pred)\n",
    "print \"F1 score on validation dataset of Bagging Knn:     \\t\", metrics.f1_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bagging Method with DT\n",
    "\n",
    "\n",
    "Using Best DT with 'entropy', each built on random subsets of 50% of the samples and 50% of the features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "clf_dt_BGDT = BaggingClassifier(DecisionTreeClassifier(criterion='entropy', min_samples_split=2),\n",
    "                             max_samples=0.5, max_features=0.5)\n",
    "start_time = time.time()\n",
    "clf_dt_BGDT.fit(X_train, y_train)\n",
    "print(\"The time for training Bagging DT is  %s seconds \" % (time.time() - start_time))\n",
    "\n",
    "# Make a prediction\n",
    "y_pred = clf_dt_BGDT.predict(X_test)\n",
    "\n",
    "print \"===================================================================\"\n",
    "print \"The accuracy on validation dataset of Bagging DT: \\t\", metrics.accuracy_score(y_test, y_pred)\n",
    "print \"Precision on validation dataset of Bagging DT:    \\t\", metrics.precision_score(y_test, y_pred)\n",
    "print \"Recall on validation dataset of Bagging DT :      \\t\", metrics.recall_score(y_test, y_pred)\n",
    "print \"F1 score on validation dataset of Bagging DT:     \\t\", metrics.f1_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest\n",
    "DT with 'gini'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "clf_dt_RF = RandomForestClassifier()\n",
    "#Training\n",
    "start_time = time.time()\n",
    "clf_dt_RF.fit(X_train, y_train)\n",
    "print(\"The time for training Decision Tree is  %s seconds \" % (time.time() - start_time))\n",
    "\n",
    "# Make a prediction\n",
    "y_pred = clf_dt_RF.predict(X_test)\n",
    "\n",
    "print \"===================================================================\"\n",
    "print \"The accuracy on validation dataset of Random Forest: \\t\", metrics.accuracy_score(y_test, y_pred)\n",
    "print \"Precision on validation dataset of Random Forest:    \\t\", metrics.precision_score(y_test, y_pred)\n",
    "print \"Recall on validation dataset of Random Forest :      \\t\", metrics.recall_score(y_test, y_pred)\n",
    "print \"F1 score on validation dataset of Random Forest:     \\t\", metrics.f1_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ada Boost\n",
    "DT with 'gini'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "clf_dt_AD = AdaBoostClassifier()\n",
    "#Training\n",
    "start_time = time.time()\n",
    "clf_dt_AD.fit(X_train, y_train)\n",
    "print(\"The time for training Decision Tree is  %s seconds \" % (time.time() - start_time))\n",
    "\n",
    "# Make a prediction\n",
    "y_pred = clf_dt_AD.predict(X_test)\n",
    "\n",
    "print \"===================================================================\"\n",
    "print \"The accuracy on validation dataset of Ada Boost: \\t\", metrics.accuracy_score(y_test, y_pred)\n",
    "print \"Precision on validation dataset of Ada Boost:    \\t\", metrics.precision_score(y_test, y_pred)\n",
    "print \"Recall on validation dataset of Ada Boost :      \\t\", metrics.recall_score(y_test, y_pred)\n",
    "print \"F1 score on validation dataset of Ada Boost:     \\t\", metrics.f1_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Naive Bayes methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "clf_NB = GaussianNB()\n",
    "#Training\n",
    "start_time = time.time()\n",
    "clf_NB.fit(X_train, y_train)\n",
    "print(\"The time for training Naive Bayes is  %s seconds \" % (time.time() - start_time))\n",
    "\n",
    "# Make a prediction\n",
    "y_pred = clf_NB.predict(X_test)\n",
    "\n",
    "print \"===================================================================\"\n",
    "print \"The accuracy on validation dataset of Naive Bayes: \\t\", metrics.accuracy_score(y_test, y_pred)\n",
    "print \"Precision on validation dataset of Naive Bayes:    \\t\", metrics.precision_score(y_test, y_pred)\n",
    "print \"Recall on validation dataset of Naive Bayes :      \\t\", metrics.recall_score(y_test, y_pred)\n",
    "print \"F1 score on validation dataset of Naive Bayes:     \\t\", metrics.f1_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Discriminant Analysis classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "clf_dt_LDA = LinearDiscriminantAnalysis()\n",
    "#Training\n",
    "start_time = time.time()\n",
    "clf_dt_LDA.fit(X_train, y_train)\n",
    "print(\"The time for training Decision Tree is  %s seconds \" % (time.time() - start_time))\n",
    "\n",
    "# Make a prediction\n",
    "y_pred = clf_dt_LDA.predict(X_test)\n",
    "\n",
    "print \"===================================================================\"\n",
    "print \"The accuracy on validation dataset of Linear Discriminant Analysis: \\t\", metrics.accuracy_score(y_test, y_pred)\n",
    "print \"Precision on validation dataset of Linear Discriminant Analysis:    \\t\", metrics.precision_score(y_test, y_pred)\n",
    "print \"Recall on validation dataset of Linear Discriminant Analysis :      \\t\", metrics.recall_score(y_test, y_pred)\n",
    "print \"F1 score on validation dataset of Linear Discriminant Analysis:     \\t\", metrics.f1_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quadratic Discriminant Analysis classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n",
    "clf_dt_QDA = QuadraticDiscriminantAnalysis()\n",
    "#Training\n",
    "start_time = time.time()\n",
    "clf_dt_QDA.fit(X_train, y_train)\n",
    "print(\"The time for training Decision Tree is  %s seconds \" % (time.time() - start_time))\n",
    "\n",
    "# Make a prediction\n",
    "y_pred = clf_dt_QDA.predict(X_test)\n",
    "\n",
    "print \"===================================================================\"\n",
    "print \"The accuracy on validation dataset of Quadratic Discriminant Analysis: \\t\", metrics.accuracy_score(y_test, y_pred)\n",
    "print \"Precision on validation dataset of Quadratic Discriminant Analysis:    \\t\", metrics.precision_score(y_test, y_pred)\n",
    "print \"Recall on validation dataset of Quadratic Discriminant Analysis :      \\t\", metrics.recall_score(y_test, y_pred)\n",
    "print \"F1 score on validation dataset of Quadratic Discriminant Analysis:     \\t\", metrics.f1_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Networks  classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Create  with Keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Dropout\n",
    "from keras.regularizers import l2, activity_l2\n",
    "import numpy\n",
    "\n",
    "# create model\n",
    "model = Sequential()\n",
    "model.add(Dense(40, input_dim=57, init='uniform', activation='relu')) # sigmoid, relu, tanh, W_regularizer=l2(0.01)\n",
    "model.add(Dropout(0.25))\n",
    "#model.add(Dense(40, init='uniform', activation='relu'))\n",
    "#model.add(Dropout(0.25))\n",
    "model.add(Dense(1, init='uniform', activation='sigmoid'))\n",
    "# Compile model\n",
    "'''\n",
    "sgd = SGD(lr=0.1, decay=1e-6, momentum=0.9, nesterov=True)\n",
    "model.compile(loss='mean_squared_error', optimizer=sgd)\n",
    "model.add(Dense(64, input_dim=64, W_regularizer=l2(0.01), activity_regularizer=activity_l2(0.01)))\n",
    "\n",
    "'''\n",
    "# keras.optimizers.RMSprop(lr=0.001, rho=0.9, epsilon=1e-08)\n",
    "# keras.optimizers.Adagrad(lr=0.01, epsilon=1e-08)\n",
    "# keras.optimizers.Adadelta(lr=1.0, rho=0.95, epsilon=1e-08)\n",
    "# keras.optimizers.Adam(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=1e-08)\n",
    "# keras.optimizers.Adamax(lr=0.002, beta_1=0.9, beta_2=0.999, epsilon=1e-08)\n",
    "\n",
    "model.compile(loss='binary_crossentropy', optimizer='rmsprop', metrics=['accuracy']) # rmsprop, adam\n",
    "# Fit the model\n",
    "start_time = time.time()\n",
    "model.fit(X_train, y_train, nb_epoch=100, batch_size=10, verbose=0)\n",
    "print(\"The time for training NN is  %s seconds \" % (time.time() - start_time))\n",
    "\n",
    "# evaluate the model\n",
    "print 'The evaluation :'\n",
    "scores = model.evaluate(X_test, y_test)\n",
    "print(\"%s: %.2f%%\" % (model.metrics_names[1], scores[1]*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "y_pred = model.predict_classes(X_test)\n",
    "print \"===================================================================\"\n",
    "print \"The accuracy on validation dataset of Neural Network: \\t\", metrics.accuracy_score(y_test, y_pred)\n",
    "print \"Precision on validation dataset of Neural Network:    \\t\", metrics.precision_score(y_test, y_pred)\n",
    "print \"Recall on validation dataset of Neural Network :      \\t\", metrics.recall_score(y_test, y_pred)\n",
    "print \"F1 score on validation dataset of Neural Network:     \\t\", metrics.f1_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#model.save_weights('pimaModelbest.hdf5',overwrite=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#np.save('Training', X_train)\n",
    "#np.save('Training', y_train)\n",
    "#np.save('validation', X_test)\n",
    "#np.save('Training', y_test)\n",
    "#np.save('Testing', X_deploy)\n",
    "#np.save('Training', y_deploy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
